\input{header}

\subsection{BH.9.50}

This simulation exercise is based on BH.9.50. Please read the exercise first. Do the exercise while thinking about the code.

\begin{exercise}
Here is some code to simulate $N$.  Explain the non-trivial steps. (By now you should have an idea about boiler-plate code and what code is specific for the problem.)

\begin{minted}[]{python}
import numpy as np
from scipy.stats import poisson, expon

np.random.seed(3)

num = 100
N = np.zeros(num)
Labda = expon(scale=1)
labdas = Labda.rvs(num)
for i in range(num):
    labda = labdas[i]
    N[i] = poisson(labda).rvs()

print(N.mean(), N.var())
adam = Labda.mean()
eve = Labda.mean() + Labda.var()
print(adam, eve)
\end{minted}

\begin{minted}[]{R}
library(ggplot2)
library(matrixStats)

set.seed(3)

num = 100
N = rep(0,num)
labdas = rexp(num, 1) 

for(i in 1:num){
  labda = labdas[i]
  N[i] = rpois(1,labda)
}

adam = 1/1
eve = 1/1 + 1/(1 * 1)
print(paste(adam, eve))
\end{minted}

\end{exercise}

\begin{exercise}
Run the above code and compare the result to the theoretical value. What do you observe? Rerun the code with \mintinline{python}{num = 10000}. Does this result in a better estimate? What do you learn from this?
\end{exercise}

\begin{exercise}
The next step is to estimate the mean and variance of the total claim size $S$.
Here is the code, run it and explain the relevant parts.
\begin{minted}{python}
import numpy as np
from scipy.stats import poisson, expon, lognorm

np.random.seed(3)

num = 100
Labda = expon(scale=1)
labdas = Labda.rvs(num)
mu, sigma = 3, 1
X = lognorm(loc=mu, s=sigma)
S = np.zeros(num)
for i in range(num):
    N = poisson(labdas[i]).rvs()
    S[i] = X.rvs(N).sum()

print(f"{S.mean()=}")
adam = X.mean() * Labda.mean()
print("mean_theoretical: ", adam)
print(f"{S.var()=}")
eve = Labda.mean() * X.var() + X.mean() ** 2 * (Labda.mean() + Labda.var())
print("var_theoretical: ", eve)
\end{minted}
\end{exercise}

\begin{minted}[]{R}
set.seed(3)

num = 100
labdas = rexp(num, 1) 
mu = 3
sigma = 1
S = rep(0,num)

for(i in 1:num){
  N = rpois(1,labdas[i])
  S[i] = sum(rlnorm(N, mean = 0, sd = sigma)+ mu)
}

theta = exp(sigma * sigma / 2) 
var = (theta * theta) * (exp(sigma * sigma) - 1)

print(paste("S.mean()=", mean(S)))
adam = (theta + mu) * (1/1)
print(paste("mean_theoretical: ", adam))
print(paste("S.var()=", var(S)))
eve = (1/1) * var + (theta + mu) ** 2 * (1/1 + 1/1/1)
print(paste("var_theoretical: ", eve))
\end{minted}

\begin{exercise}
Here is the code to plot the distribution. Change $\lambda$ such that it is $\sim\Exp{1/2}$ and include the figure.
\begin{minted}{python}
import numpy as np
from scipy.stats import poisson, expon
import matplotlib.pyplot as plt

np.random.seed(3)

num = 100
N = np.zeros(num)
Labda = expon(scale=1)
labdas = Labda.rvs(num)
for i in range(num):
    labda = labdas[i]
    N[i] = poisson(labda).rvs()

plt.hist(N, density=True)
plt.savefig("figures/bh-9.50-fig.pdf")
\end{minted}
\end{exercise}

\begin{minted}[]{R}
set.seed(3)

num = 100
N = rep(0,num)
labdas = rexp(num, 1)
for(i in 1:num){
  labda = labdas[i]
  N[i] = rpois(1,labda)
}

pdf(file="figures/bh-9.50-fig.pdf",
    width = 8, height = 7, bg = "white",          
    colormodel = "cmyk", paper = "A4")

df1 = data.frame(N)
ggplot()+
  geom_histogram(df1, mapping = (aes(x = N, y = ..density..)))

dev.off()
\end{minted}

\begin{exercise}
Finally, let's do the integration numerically to compute $\P{N=3}$. Explain the code. Why, in particular, do I compare the result against \mintinline{python}{geom(1/2).pmf(4)}? (Hint, read the documentation of \mintinline{python}{scipy.stats.geom} (\mintinline{R}{dgeom()} function in R) to understand why.)
\begin{minted}{python}
import numpy as np
from scipy.integrate import quad
from scipy.stats import expon, geom

Labda = expon(scale=1)

def integrand(labda):
    return np.exp(-labda) * labda ** 3 / 6 * Labda.pdf(labda)

res = quad(lambda labda: integrand(labda), 0, np.infty)
print(res)
print(geom(1 / 2).pmf(3))
print(geom(1 / 2).pmf(4))
\end{minted}

\begin{minted}[]{R}
library(cubature)  

integrand = function(labda){
  return (exp(-labda) * labda ** 3 / 6 * exp(-labda) )
}

res = adaptIntegrate(function(x){integrand(x[1])}, lowerLimit = 0, upperLimit = Inf)
print(res)
print(dgeom(3 - 1, 1 / 2))
print(dgeom(4 - 1, 1 / 2))
\end{minted}

\end{exercise}





\input{trailer}
