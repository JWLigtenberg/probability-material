% arara: pdflatex: { shell: yes }
% arara: pythontex: {verbose: yes, rerun: modified }
% arara: pdflatex: { shell: yes }

\input{header.tex}
\chapter{Chapter 8: Exercises and remarks}

\section{Section 8.1}


\begin{exercise}
In probability theory we often want to study properties of  functions of rvs. Provide an example for such  a function.
\begin{solution}
Recall that $\V{X} = \E{X^2}-(\E X)^2$; so we have to deal with the function $g(x) = x^{2}$ because $\E{X^2}= \E{g(X)}$.
Note that even to properly define the variance, we have to deal with a function that is not one-to-one everywhere on $\R$.
\end{solution}
\end{exercise}


\begin{exercise}
Let the rv $X$ be uniform on the set $\{0, 1, 2, 3, 4, 5\}$. Derive the PMF and the CDF of $Z=3X$. Explicitly specify the domain.
\begin{solution}
  \begin{align}
    \label{eq:17}
    X &\in \{0, \ldots, 5\} \implies Z \in \{0, 3, 6, 9, 12, 15\}, \quad \text{and not in } \{0, 1, 2, \ldots, 14, 15\},\\
z &= g(x) = 3x,\\
p_{Z}(z) &= \sum_{x: g(x) =z } p_{X}(x) = \frac{1}{6} \1{z \in \{0, 3, 6, 9, 12, 15\}}, \\
F_{Z}(z) &=\frac{1}{6} \sum_{x=0}^{z} \1{x \in \{0, 3, 6, 9, 12, 15\}}.
  \end{align}
\end{solution}
\end{exercise}



\begin{exercise}
Suppose $y=g(x)$ for some differentiable function $g$. We like to express the PDF $f_{Y}$ for $Y=g(X)$ in terms of the PDF $f_X$ and $g$.
This is easy  when $g$ is strictly increasing and has an inverse at $y$, because
\begin{align}\label{eq:1D-CDF}
F_{Y}(y) = \P{Y\leq y} = \P{g(X) \leq y} = \P{X\leq g^{-1}(y)} = F_X(g^{-1}(y)).
\end{align}
Now we  take the derivative at the LHS and RHS to get with the chain rule
\begin{align*}
f_{Y}(y) = F_{Y}'(y) = \frac{\d }{\d y} F_{X}(g^{-1}(y)) = f_{X}(g^{-1}(y)) \frac{\d }{\d y} g^{-1}(y) = f_{X}(x) \frac{1}{g'(x)},
\end{align*}
where we write $x=g^{-1}(y)$ in the last step.
But why is the derivative of $g^{-1}(y)$ at $y$ equal to $1/g'(x)$, with $x=g^{-1}(y)$?
\begin{solution}
To get the derivative of $g^{-1}$, consider the equality $g(g^{-1}(y)) = y$. Then, taking derivatives with respect to $y$ at both sides, and  applying the chain rule,
\begin{align*}
g(g^{-1}(y)) = y \implies \frac{\d}{\d y} g(g^{-1}(y)) =1 \iff g'(g^{-1}(y)) \frac{\d}{\d y} g^{-1}(y) = 1 \implies  \frac{\d}{\d y} g^{-1}(y)  = 1 /g'(x),
\end{align*}
where we use that $g^{-1}(y)=x$. Notice why $g$ is assumed increasing: now we know that $g'(x)\neq0$.
\end{solution}
\end{exercise}



\begin{exercise}
When $g$ is not strictly increasing everywhere, there can be no or multiple points $x$ such that $g(x)=y$.
Explain that in such cases it is much more difficult to  express $F_Y$ in terms of $F_X$ than directly use the densities (assuming that $g$ is differentiable).  Extend your reasoning to 2D.
\begin{solution}
When working the CDFs, we need to solve the problem $\{x : g(x) \leq y\}$. If we take $g(x) = \sin x + x/100$ then this is really messy. In fact, to solve this, we first solve for the set ${x : g(x) = y}$, which might still be hard, but requires less work than check each and every interval.

With PDFs  we only have to require \emph{locally} that $g$ is one-to-one, and we don't have to work with inequalities, but can direcly focus on the set $\{x : g(x) = y\}$.

In 2D,  functions can have saddle points, i.e., points in which the function increases in one direction and decreases in another.
Then finding the set of points $x$ such that\footnote{} $g(x,y)\leq (u,v)$ (which we need if we want to express $\P{g(X, Y) \leq (u,v)}$ in terms of the distribution $F_{X,Y}$) is not a particularly attractive task, to say the least.

See also the inverse function theorem, which will be covered in more detail next block.
\end{solution}
\end{exercise}


\begin{exercise}
The general 1D change of variables formula is like this,
\begin{align*}
f_{Y}(y) = \sum_{x_{i}: g(x_{i})=y}f_{X}(x_{i})\frac 1{|g'(x_{i})|},
\end{align*}
with some natural conditions on $g$.
Apply this formula to the case $g(x)= x^{2}$.
\begin{solution}
Note that $g(x)=x^2$ is not monotone increasing, moreover, $g^{-1}(y)$  does not exist (in $\R$) for $y< 0$.
We  split the line into disjoint intervals in which $g$ is either strictly increasing or decreasing, and then we apply the above rule in each of the intervals. Since  $g'(x)= 2x$ and $x=\pm \sqrt y$,
\begin{align*}
f_{Y}(y) = f_{X}(\sqrt{y})\frac 1{2\sqrt y} + f_{X}(-\sqrt{y})\frac 1{2\sqrt y}.
\end{align*}
\end{solution}
\end{exercise}

\begin{exercise} If $X\sim \Exp{1}$,  use  the change-of-variables theorem to obtain the density of $Y=g(X)=\lambda X$. What is $\E Y$?
\begin{solution}
Take $y= g(x)= \lambda x$. Then,
\begin{align*}
f_{Y}(y) = f_X(x)\frac{\d x}{\d y} = f_{X}(x) \frac{1}{g'(x)} = e^{-y/\lambda} \frac 1 \lambda.
\end{align*}
With this,
\begin{align*}
\E Y = \int_0^{\infty} y f_Y(y) \d y = \int_0^{\infty} y e^{-y/\lambda} \frac 1 \lambda  \d y.
\end{align*}
To solve this integral, I recognize $y/\lambda$ in the exponent, and I want to get rid of the $1/\lambda$ factor. Hence, I write $u=y/\lambda$, and use this to see that
\begin{align*}
u = y/\lambda \implies \d u = \d y /\lambda \implies \d y = \lambda \d u.
\end{align*}
Then, including $a$ and $b$ for the boundaries to show explicitly what is going on when changing the variables
\begin{align*}
\int_{a}^b y/\lambda e^{-y/\lambda} \d y =
\int_{a/\lambda }^{b/\lambda } u e^{-u} \lambda \d u = \lambda \int_{a/\lambda }^{b/\lambda } u e^{-u}  \d u.
\end{align*}
Applying this to our case so that $a = 0/\lambda = 0$ and $b = \infty/\lambda = \infty$,
\begin{align*}
\E Y = \lambda \int_{0 }^{\infty } u e^{-u}  \d u = \lambda \E X.
\end{align*}
\end{solution}
\end{exercise}

\begin{exercise}
Show that the 1D change-of-variables formula relates directly to the substitution rule of integration theory to solve 1D integrals.
\begin{solution}
When we have the density  $f_{Y}$ and the function $g$, then the substitution rule says that,
\begin{align*}
\int_{a}^b f_{Y}(g(x)) g'(x)\d x = \int_{g(a)}^{g(b)} f_{Y}(y) \d y.
\end{align*}
We also want that the transformation from $X$ to $Y$ does not affect the probability of the set (event) $A = [a,b]$, hence,
\begin{align*}
\int_{g(a)}^{g(b)} f_{Y}(y) \d y = \int_{a}^{b} f_{X}(x) \d x.
\end{align*}
Combining the above two equations gives that
\begin{align*}
\int_{a}^{b} f_{Y}(g(x))) g'(x) \d x = \int_{a}^{b} f_{X}(x) \d x.
\end{align*}
Since this holds for any $a$ and $b$, it follows that
\begin{align*}
f_{Y}(g(x)) g'(x) = f_{X}(x).
\end{align*}
\end{solution}
\end{exercise}

\begin{exercise}
Use the change of variable formula to relate the $\Geo{p}$ and the $\FS{p}$ distributions.
\begin{solution}
If $X$ follows a geometric distribution and we have $Y=X+1$, then $Y$ follows a first success distribution. This is just re-indexing, as you will also find with the change of variables formula.
\end{solution}
\end{exercise}

\begin{exercise}
BH.8.1.1 write that `The support of $Y$ is all $g(x)$ with $x$ in the support of $X$.' Do they say that $\supp{Y} = \{x : g(x) \in \supp{X}$? BTW, what is the difference between $\supp{X}$ and $\sup{X}$?
\begin{hint}
  $\sup{}$ stands for supremum, $\supp{}$ stands for support.
\end{hint}
\begin{solution}
They say that $\supp{Y} = \{g(x):x\in\supp{X}\}$. The support of a function $f$ is defined as $\supp{f} = \{\omega:f(\omega)\neq0\}$. The supremum of a set is the least upper bound of that set, and the supremum of a function would be the supremum of its range.
\end{solution}
\end{exercise}


\begin{exercise}
BH.8.1.3. Check how all moments were found.
\begin{solution}
See page 284 of BH.
\end{solution}
\end{exercise}

\begin{exercise}
Let $X \sim \Unif{0, 5}$. Using the one dimensional change of variables theorem (BH.8.1.1),  derive the PDF and the CDF of $Z=3X$. Explicitly specify the domain.
\begin{solution}
  \begin{align}
X &\in [0, 5]  \implies Z \in [0, 15],\\
z &= 3x = g(x) \implies x = z/3, \\
f_{Z}(z) &= f_{X}(x) \frac{\d x}{\d z},\\
\frac{\d z}{\d x} &= 3,\\
f_{Z}(z) &= f_{X}(z/3) \frac{1}{3}.
  \end{align}
$F_{Z}(u) = 1$ for $u\geq 15$ and $F_{Z}(u) = 0$ for $u\leq 0$. When $0\leq u \leq 15$,
  \begin{align}
  F_{Z}(u) &= \int_{0}^{u} f_{X}(z/3) \frac{1}{3} \d z = \frac{1}{5}\int_{0}^{u} \1{0\leq  z/3\leq 5}  \frac{1}{3} \d z \\
&= \frac{1}{5}\int_{0}^{u} \1{0\leq z\leq 15}  \frac{1}{3} \d z = \frac{u}{15}.
  \end{align}
\end{solution}
\end{exercise}


\begin{exercise}
When $Z=X^3$ and $X\sim \Unif{0,5}$, using the one dimensional change of variables theorem to  derive the PDF and the CDF of $Z$. Specify the domain of $Z$.
\begin{solution}
\begin{align}
X &\in [0, 5]  \implies Z \in [0, 125],\\
z &= x^{3} = g(x) \implies x = z^{1/3}, \\
f_{Z}(z) &= f_{X}(x) \frac{\d x}{\d z},\\
\frac{\d z}{\d x} &= 3 x^{2} = 3 z^{2/3},\\
f_{Z}(z) &= f_{X}(z^{1/3}) \frac{1}{3z^{2/3}}.
 \end{align}
When $F_{Z}(u) = 1$ for $u\geq 125$ and $F_{Z}(u) = 0$ for $u\leq 0$. When $0\leq u \leq 125$,
 \begin{align}
 F_{Z}(u) &= \int_{0}^{u} f_{X}(z^{1/3}) \frac{1}{3z^{2/3}} \d z = \frac{1}{5}\int_{0}^{u} \1{0\leq  z^{1/3}\leq 5}  \frac{1}{3z^{2/3}} \d z \\
&= \frac{1}{5}\int_{0}^{u} \1{0\leq z\leq 125}  \frac{1}{3z^{2/3}} \d z \\
&= \frac{1}{5}\int_{0}^{u}  \frac{1}{3z^{2/3}} \d z =  \frac{1}{5}z^{1/3}\biggr|_{0}^{u} = u^{1/3}/5.
 \end{align}
\end{solution}
\end{exercise}



\begin{exercise}
Let $X \sim \Norm{\mu,\sigma^2}$. Using the one dimensional change of variables theorem  BH.8.1.1, show that $Z = \frac{X-\mu}{\sigma}\sim \Norm{0, 1}$.
\begin{solution}
  \begin{align}
    \label{eq:18}
    z &= g(x) = (x-\mu)/\sigma, \implies x = \sigma z + \mu\\
f_{Z}(z) &= f_{X}(x) \frac{\d x}{\d z}, \\
\frac{\d z}{\d x } &= \frac{1}{\sigma}, \\
f_{Z}(z) &= f_{X}(x) \sigma = \sigma f_{X}(\sigma z + \mu) \\
\intertext{and now using the density of $X\sim \Norm{\mu, \sigma$},}
f_{Z}(z) &=\frac1{\sqrt{2\pi} \sigma } e^{-(\sigma z + \mu -\mu)^{2}/2\sigma^{2}} \sigma = \frac1{\sqrt{2\pi}} e^{-z^{2}/2}.
  \end{align}
\end{solution}
\end{exercise}

\begin{exercise}
Let $X \sim \Exp{1}$. Derive the PDF of $e^{-X}$.
\begin{solution}
  \begin{align}
z &= g(x) = e^{-x} \implies x = - \log z,\\
x &\in (0, \infty) \implies z \in (0, 1),\\
f_{Z}(z) &= f_{X}(x) \frac{\d x}{\d z}, \\
\frac{\d z}{\d x } &= - e^{-x}, \quad \text{Don't forget to take the abs value next}, \\
f_{Z}(z) &= f_{X}(x) e^{x} = e^{-x} e^{x} = 1 \1{0< z < 1},
  \end{align}
where we include the domain of $Z$ in the last equality.
\end{solution}
\end{exercise}

\begin{exercise}
Let $X, Y$ be iid standard normal. Using the $n$-dimensional change of variables theorem, derive the joint PDF of $(X+Y, X-Y)$.

Check your final answer using BH.7.5.8.
\begin{solution}
\begin{align}
(u,v) &= (x+y, x-y) = g(x,y) \implies (x,y) = ((u+v)/2, (u-v)/2), \\
\frac{\partial (u,v)}{\partial (x, y)} &=
  \begin{vmatrix}
    1 & 1 \\
1 & -1
  \end{vmatrix} = -2 \implies |-2| = 2,\\
f_{U,V}(u,v) &= f_{X,Y}(x,y) \frac{\partial(x,y)}{\partial(u,v)} = f_{X,Y}((u+v)/2, (u-v)/2) /2 \\
&= \frac{1}{4\pi} e^{-((u+v)/2)^{2}/2} e^{-((u-v)/2)^{2}/2} \\
&= \frac{1}{4\pi} e^{-u^{2}/4-v^{2}/4},
\end{align}
where we work out the squares and simplify.
Hence, $U$ and $V$ are independent and normally distributed with mean 0 and $\sigma=\sqrt 2$.
This is in line with our earlier definition of a multi-variate normal distribution.
\end{solution}

\end{exercise}


\begin{exercise}
Specify the domain of the new random variable for the following transformations; this important aspect of the change of variables is often overlooked.
Let $U$, $V$, $W$, $X$, $X_1$, $X_2$, $Y$ and $Z$ be rvs and let $a$, $b$ and $c$ be arbitrary constants.
\begin{enumerate}
    \item $Z = Y^{4}$ for $Y \in (-\infty,\infty)$;
    \item $Y = X^{3}+a$ for $X \in (0,1)$;
    \item $U = |V|+b$ for $V \in (-\infty,\infty)$;
    \item $Y = e^{X^3}$ for $X \in (-\infty,\infty)$;
    \item $V = U \1{U \leq c}$ for $U \in (-\infty,\infty)$;
    \item $Y = \sin(X)$ for $X \in (-\infty,\infty)$;
    \item $Y = \frac{X_1}{X_1+X_2}$ for $X_1 \in (0,\infty)$ and $X_2 \in (0,\infty)$;
    \item $Z = \log(UV)$ for $U \in (0,\infty)$ and $V \in (0,\infty)$. \\
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item $Z = Y^{4} \in [0, \infty)$ for $Y \in (-\infty,\infty)$;
    \item $Y = X^{3}+a \in (a, a+1)$ for $X \in (0,1)$;
    \item $U = |V|+b \in [b, \infty)$ for $V \in (-\infty,\infty)$;
    \item $Y = e^{X^3} \in (0, \infty)$ for $X \in (-\infty,\infty)$;
    \item $V = U \1{U \leq c} \in (-\infty, c]$ for $U \in (-\infty,\infty)$;
    \item $Y = \sin(X) \in [-1, 1]$ for $X \in (-\infty,\infty)$;
    \item $Y = \frac{X_1}{X_1+X_2} \in (0, 1)$ for $X_1 \in (0,\infty)$ and $X_2 \in (0,\infty)$;
    \item $Z = \log(UV) \in (-\infty, \infty)$ for $U \in (0,\infty)$ and $V \in (0,\infty)$. \\
\end{enumerate}
\end{solution}

\end{exercise}

\begin{exercise}
When adding a different equality, we need to be careful to not create a functional relationship between our two new variables $U,V$, for example $U=X+Y$ and $V=\sin(X+Y)$, or $U=\frac X Y$ and $V = \frac Y X$ for conforming $X,Y$.
What would happen to the determinant of the Jacobian matrix if we did?
Why would this happen?
Explain in your own words.
\begin{solution}
  When the variables become dependent, the Jacobian becomes zero. For instance,  in the latter case,
\begin{align}
\frac{\partial (u,v)}{\partial (x, y)} =
  \begin{vmatrix}
    1/y & -x/y^{2} \\
-y/x^{2} & 1/x
  \end{vmatrix} = \frac{1}{x y} - \frac{x}{y^2}\frac{y}{x^{2}} = 0.
\end{align}
Moreover, the function $g$ is not locally one-to-one.

\end{solution}
\end{exercise}


\section{Section 8.2}
\label{sec:section-8.2}


\begin{exercise}
To find the distribution of a convolution through the change of variables formula, we seem to need to add a `redundant' equality?
But why is that?
What would be the problem if we do not add this?
Explain in your own words.
\begin{solution}
If we would not add this extra variable, we cannot use the change of variables theorem. We also need a function to deal with the scaling. In the change of variables theorem, this is the Jacobian.

There is also another problem.
Consider the function $g(x, y)$ that maps $\R^{2}$ to $\R$.
The inverse set $\{ (x,y) : g(x,y) =z \}$ can be quite complicated, while the set $\{y : g(x, y) = z\}$ for a fixed $x$ is hopefully just one point.
Hence, the mapping $(x, y) \to (x, g(x,y))$ is, at least locally, one-to-one.

It is possible to deal with the more general problem, but this requires much more theory than we need for this course.
\end{solution}
\end{exercise}


\begin{exercise}
In this exercise, we combine what we learned in BH.8.1.4 and  BH.8.1.9.  Let $S$ be the sum of two iid chi-square distributed variables (with one degree of freedom). Using just these two examples, show that $S \sim \Exp{1/2}$.

\begin{hint}
Let $X, Y$ be iid standard normal. Since the square of a standard normal r.v. is chi-square distributed, we can write $S$ as $S=X^2+Y^2$ (here we use  BH.8.1.4).
\end{hint}
\begin{solution}
  From BH.8.1.4: $Z$ chi-square $\implies X=\sqrt{Z} \sim \Norm{0,1}$.
  Then, from BH.8.1.9,
\begin{equation}
X^{2}+Y^2 = (\sqrt{2T} \cos U)^2 + (\sqrt{2T} \sin U)^2 = 2T \left(\cos^2 U+\sin^2 U\right) = 2T \sim \Exp{1/2},
\end{equation}
 when $X, Y\sim \Norm{0,1}$.
\end{solution}
\end{exercise}



\begin{exercise}
A student has obtained an iid
random sample of size 2 from a Cauchy distribution. Let the rvs $X$ and $Y$ model the  values of the first and second sample.
Since s/he does not know what the mean of a Cauchy distribution is, s/he wants to average the sample to obtain what she thinks is a good estimate of the true mean.

To find the distribution of this sample mean, we need to find an expression for $f_W(w)$, where $W=\frac{X+Y}2$.

\begin{enumerate}
\item Find an expression for $f_W(w)$ in the form of an integral, but do not solve it.
\item It turns out that if we solve the integral, we get that $f_W(w) = f_X(w)$. The distribution of our sample mean is still Cauchy; we did not obtain a better estimate of the Cauchy mean by calculating the sample mean!

Explain (in your own words) why this makes sense.
\end{enumerate}
\begin{solution}
 Take $g(x,y) = (x, w) = (x, (x+y)/2)$. Then, $y=2w -x$.
\begin{align}
\frac{\partial (x,w)}{\partial (x, y)} &=
  \begin{vmatrix}
    1 & 0 \\
1/2 & 1/2
  \end{vmatrix} = 1/2,\\
f_{X,W}(x,w) &= f_{X,Y}(x,y) \frac{\partial(x,y)}{\partial(x,w)} = \frac{1}{\pi(1+x^{2})}\frac{1}{\pi(1+(2w-x)^{2})} 2,\\
f_{W}(w) &= \int_{-\infty}^{\infty} f_{X,W}(x,w) \d x = \frac{2}{\pi^{2}} \int_{-\infty}^{\infty} \frac{1}{1+x^{2}}\frac{1}{1+(2w-x)^{2}} \d x.
\end{align}

The expectation of a Cauchy distributed r.v. $X$ is not well-defined because $\E{|X|} = \infty$. As a consequence, taking the average of some outcomes (i.e. a sample) will also not give a sensible answer.
\end{solution}
\end{exercise}


\section{Section 8.3}
\label{sec:section-8.3}

\begin{exercise}
If $a=b=1$, why is $\Beta{a,b}=\Unif{[0,1]]}$?
\begin{solution}
When $a=b=1$, the probability density functions are the same, hence the distributions are the same.
\end{solution}
\end{exercise}


\begin{exercise}
If $a=b$, why is $\Beta{a,b}$ symmetric?
\begin{solution}
Because $\beta(a,b)$ and $x^a(1-x)^b$ are (around $\frac12$).
\end{solution}
\end{exercise}

\begin{exercise}
If $a>b$ and  $X\sim\Beta{a,b}$, is $\E X > 1/2$?
\begin{solution}
$\E{X} = \frac{a}{a+b}$.
\end{solution}
\end{exercise}

\begin{exercise}
BH.8.3.2, last equation. How do the authors get to the equation
\begin{equation*}
\beta(a,b) = \frac{1}{(a+b-1) {a+b-2\choose a-1}}?
\end{equation*}
\begin{solution}
They notice that $\int_0^1\binom n k p^k(1-p)^{n-k}\d p = \frac1{n+1}$ for all $0\leq k\leq n$. Then filling in $k=a-1,n=b-1$, the claim follows.
\end{solution}
\end{exercise}

\begin{exercise}
BH.8.3.3. The authors write that $X$ is not marginally Binomial, but is conditionally Binomial. What is the difference?
\begin{solution}
A marginal distribution is simply a distribution, called marginal to emphasize is was marginalized out from another, joint, distribution. A conditional distribution is a distribution given some restrictions from another random variable.
\end{solution}
\end{exercise}

\begin{exercise}
BH.8.3.3. The authors use a smart trick to find an expression for the posterior distribution $f_{p|X=k}$ of $p$. Use this posterior to derive an expression for $\P{X=k}$ by using the fact that
\begin{equation*}
\P{X=k} = \frac{{n\choose k} p^{k}(1-p)^{n-k} \Beta{p;a,b}}{\Beta{p;a+k, b+n-k}},
\end{equation*}
and simplyfing the RHS.
\begin{solution}
This is just some algebra, look up the beta-binomial distribution to see if you got it right.
\end{solution}
\end{exercise}

\begin{exercise}
BH.8.3.3. Why does $a-1$ correspond to  the number of prior successes, in other words, why is it not $a$, but $a-1$?
\begin{solution}
If we have no prior successes, it makes most sense for our prior to be uniform. Similarly for failures. Since the uniform distribution corresponds to $a=b=1$, we need $a-1$ to represent the number of successes.
\end{solution}
\end{exercise}

\begin{exercise}
BH.8.3.4.b. Given that the first patient is cured, what is the probability that the rest of the patients, i.e., the other $n-1$, will also be cured?
\begin{solution}
Let $X_{1,\dots,n}$ denote the random variable counting how many of the first $n$ people are cured, with similar notation for other subsets. Then, we calculate:
\begin{align*}
    \P{X_{2,\dots,n}=n-1|X_1=1} &= \frac{\P{X_{2\dots,n}=n-1, X_1=1}}{\P{X_1=1}}\\
    &= \frac{\int_0^1 \P{X_{2\dots,n}=n-1, X_1=1|p}\d p}{\int_0^1 \P{X_1=1|p}\d p}\\
    &= \frac{\int_0^1 \P{X_{2\dots,n}=n-1|p}\P{X_1=1|p}\d p}{\int_0^1 \P{X_1=1|p}\d p}\\
    &= \frac{\int_0^1 p^n\d p}{\int_0^1 p \d p}\\
    &= \frac{2}{n+1},
\end{align*}
where we use the definition of conditional probability, the law of total probability, independence given $p$, and some easy integration, in that order.
\end{solution}
\end{exercise}


\begin{exercise}
Is this claim correct? Let $T$ be the sum of two iid $\Unif{0,1}$ rvs.
Then there exist $a, b$ such that $T \sim \Beta{a,b}$. (You don't need to derive the distribution of $T$.)
\begin{solution}
Incorrect: The support of $T$ is $(0,2)$ whereas the support of any beta distribution is $(0,1)$. Hence, $T$ does not have a beta distribution for some $a,b$.\\
Also see page 378 of the book for the distribution of the sum of two uniform distributions. This might help your intuition for this solution.
\end{solution}
\end{exercise}

\begin{exercise}
Show that $\beta(1, b) = 1/b$ by integrating the PDF of the beta distribution for $a=1$. (Do not use the results of BH 8.5 for this exercise.)
\begin{solution}
We use that the PDF integrates to 1:
$$1 = \int_0^1 \frac1{\beta(1,b)} (1-x)^{b-1} \d x =  \frac1{\beta(1,b)}  \left[-\frac{1}{b}(1-x)^{b}\right]_0^1 =  \frac{1}{\beta(1,b) b}.$$
Hence, $\beta(1, b) = \frac1b$.
\end{solution}
\end{exercise}

\begin{exercise}
Let $a, b > 1$. Show that the PDF of the beta distribution attains a maximum at $x = \frac{a-1}{a+b-2}$. Explicitly indicate where the assumption that  $a, b > 1$ is used.
\begin{solution}
The scaling factor $\beta(a, b)$ is a positive constant, so we may as well leave it out and maximize $x^{a-1}(1-x)^{b-1}$. Note that its derivative (to $x$) is given by
\begin{align*}
\frac{d}{dx} x^{a-1}(1-x)^{b-1} &= ((a-1)(1-x) -(b-1)x)x^{a-2}(1-x)^{b-2} \\ &= ((a-1) - (a+b-2)x) x^{a-2}(1-x)^{b-2}.
\end{align*}
Setting this to zero yields $x = \frac{a-1}{a+b-2}$ as the only candidate for an interior optimum. Since $a, b > 1$, we have $0 < x < 1$. If  $a, b > 1$, then the PDF converges to 0 as $x \to 0$ or $x \to 1$, so then we conclude that $x = \frac{a-1}{a+b-2}$ indeed yields a maximum. (Think about this last sentence; most students do not use the information that $a,b>1$ correctly.)
\end{solution}
\end{exercise}


\begin{exercise}
Explain in your own words:
\begin{enumerate}
\item What is a prior?
\item What is a conjugate prior?
\end{enumerate}
\begin{solution}
A prior is a distribution reflecting one's information or belief about a parameter before updating it with information.

It is harder than you might think, hardly any student gives a completely satisfactory answer here. Compare your solution to the definition above. If they are different, try to understand how exactly your solution was different and determine which definition is better.

A conjugate prior is a prior distribution such that the posterior distribution is in the same family of distributions.
\end{solution}
\end{exercise}


\begin{exercise} \phantom{ }
\begin{enumerate}
\item Look up on the web: what is the conjugate prior of the multinomial distribution? Give a name and a formula.
\item Explain why the Beta distribution is a special case of this distribution.
\end{enumerate}
\begin{solution}
Dirichlet distribution. The Beta distribution is a special case of the Dirichlet distribution, because binomial is a special case of multinomial. Of course, this can also be shown directly using the formula.
\end{solution}
\end{exercise}



\begin{exercise}
You make a test with $n$ multiple choice questions and you give the correct answer to each question independently with probability $p$. The teacher's prior belief about $p$ is reflected by a uniform distribution: $p \sim \Unif{0,1}$.
Let $X$ be the number of correct answers you give.
What is the teacher's posterior distribution $p|X=k$? (You don't have to do a lot of math here; simply use a result from the book.)
\begin{solution}
The prior is $p \sim \text{Beta}(1,1)$. The posterior is $p|X=k \sim \text{Beta}(1+k,1+n-k)$.
\end{solution}
\end{exercise}

\begin{exercise}
You find a coin on the street. Initially, you are rather confident that this should be (approximately) a fair coin. This is reflected in your prior belief of the probability $p$ of heads: $p \sim \Beta{10,10}$. Your friend is a bit more skeptical and assumes a uniform prior: $p \sim \Unif{0,1}$.  You toss the coin 1000 times, and it comes up heads 900 times.
\begin{enumerate}
\item Determine your posterior distribution. (Again, use  a result from the book)
\item Determine your friend's posterior distribution.
\item Compare the means of your posterior distribution and your friend's posterior distribution. Comment on the effect of the prior distribution if you have a lot of data.
\end{enumerate}
\begin{solution}
Let $X$ denote the number of heads.
\begin{enumerate}
    \item Your posterior is $p|X=900 \sim \text{Beta}(910,110)$.
    \item Your friend's posterior is $p|X=900 \sim \text{Beta}(901,101)$.
    \item The mean of your posterior is $\frac{910}{910+110} = \frac{91}{102} \approx 0.892$; the mean of your friend's posterior is  $\frac{901}{901+101} = \frac{901}{1002} \approx 0.899$. The difference is small, so the  effect of the prior distribution is small if you have a lot of data. This effect is known as \textit{washing out the prior}.
\end{enumerate}
\end{solution}
\end{exercise}


\begin{exercise}
We have an urn with 1000 coins. One of those is biased such that $\P{H} = 99/100 = 1-\P{T}$, all others are fair. You select at random a coin, i.e., with probability $1/1000$ you select the biased one, and start throwing. You see 10 heads in row. What is the probability you picked the biased coin?
\begin{solution}
Of course, this should depend on when we see this string of outcomes: if it happens for the first time after a thousand throws then we can still confidently claim the coin is fair. Assuming this happens in the first 10 throws however, we use Bayes' theorem to find the following:
\begin{align*}
    \P{B|H_{10}} &= \frac{\P{H_{10}|B}\P{B}}{\P{H_{10}|B}\P{B}+\P{H_{10}|B^c}\P{B^c}}\\
    &= \frac{(99/100)^{10}/1000}{(99/100)^{10}/1000+(1/2)^{10}\cdot999/1000}\\
    &\approx 0.48.
\end{align*}
\end{solution}
\end{exercise}


\begin{exercise}
BH.8.3.5. write that $X_j$ is an indicator of the $j$th throw beind made. Can this be the formal definition: $X_j = \1{N\geq j}$?
\begin{solution}
No, of course not. She gets $N$ free throws, but her getting e.g. two tries in a game is not equivalent to her scoring the first one.
\end{solution}
\end{exercise}


\begin{exercise}
Use the pmf of the Beta-Binomial distribution to prove the following identity:
\begin{equation*}
   \sum_{k=0}^n \frac{\binom{n}{k}\binom{a+b-2}{a-1}  (a+b-1) }{\binom{a+b+n-2}{a+k-1}(a+b+n-1)} = 1.
\end{equation*}
for all positive integers $a, b, n$.
\begin{solution}
This  states that the PMF of the Beta-Binomial distribution, $$P(X=k) = \binom{n}{k} \frac{\beta(a+k,b+n-k)}{\beta(a,b)},$$ sums to 1. To see this, we have to rewrite the beta functions in terms of binomial coefficients:
\begin{align*}
   \frac{1}{\beta(a,b)} = \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)} = \frac{(a+b-1)!}{(a-1)! (b-1)!} = (a+b-1) \binom{a+b-2}{a-1}, \\
   \frac{1}{\beta(a+k,b+n-k)}  = (a+b+n-1) \binom{a+b+n-2}{a+k-1}.
\end{align*}
Plugging this in gives the result.
\end{solution}
\end{exercise}



\section{Section 8.4}
\label{sec:section-8.4}

\begin{exercise}
What is the SCV of $\Gamm{n, \lambda}$ distributed rv $X$?
\begin{solution}
$\V X = n/\lambda^{2}$, $\E X = n/\lambda$, SCV $=1/n$.
\end{solution}
\end{exercise}

\begin{exercise}
We have a machine that has temperature $x_0e^{-\alpha t}$ after a time $t\geq 0$. We switch the machine on after the arrival of~$q$ jobs. Job interarrival times are iid $\sim\Exp{\lambda}$. The temperature at the moment the $q$th job arrives has the distribution $x_0\exp\{-\alpha Y\}$, with $Y\sim \Gamm{q, \lambda}$. Explain why this is so.
\begin{solution}
We have to wait for $q$ exponentially distributed interarrival times. The sum of these interarrival times is $\Gamm{1, \lambda}$.
\end{solution}
\end{exercise}



\begin{exercise}
Consider the chi-square distribution (with one degreee of freedom) from BH.8.1.4.

Starting from the expression $f_Y(y) = \varphi \left(\sqrt{y}\right) y^{-1/2}$ in this example, show that this  chi-square distribution is a special case of the Gamma distribution and specify the corresponding values of the parameters $a$ and $\lambda$.

\begin{solution} We fill in $\varphi(x) = \frac{1}{\sqrt{2\pi}} e^{-x^2/2}$ to find
\begin{equation*} f_Y(y) = \varphi \left(\sqrt{y}\right) y^{-1/2} = \frac{1}{\sqrt{2\pi}} y^{-1/2} e^{-(\sqrt{y})^2/2} = \frac{1}{\sqrt{2\pi}} y^{-1/2} e^{-y/2}, \end{equation*} so $a = \tfrac12$ and $\lambda = \tfrac12$.
\end{solution}
\end{exercise}

\begin{exercise}
Is the sum of any two Gamma distributions again Gamma?
\begin{solution}
Incorrect: the scale parameters $\lambda$ need to be the same \emph{and} both random variables need to be independent.
\end{solution}
\end{exercise}

\begin{exercise}
Prove by induction that $\Gamma(n)=(n-1)!$ if $n$ is a positive integer.

\begin{solution}
The base case is $n=1$. We have $\Gamma(1) = \int_0^\infty e^{-x} \d x = 1 = 0!$, so the statement holds for $n=1$. Now let $k \in \mathbb N$ be arbitrary and assume that the statement holds for $n=k$, i.e. that $\Gamma(k) = (k-1)!$. Then \begin{equation}\Gamma(k+1) = k\Gamma(k) = k (k-1)! = k! = ((k+1)-1)!,
\end{equation}
so the statement also holds for $n=k+1$. By mathematical induction, we conclude that  $\Gamma(n)=(n-1)!$ for all positive integers $n$.
\end{solution}
\end{exercise}

\begin{exercise}
Is the Poisson distribution  the conjugate prior of the Gamma distribution?
\begin{solution}
Incorrect: It is the other way around, the Gamma distribution is the conjugate prior of the Poisson distribution. This statement doesn't make much sense, for example one would need to say for which parameter of the Gamma distribution it is the prior. In addition, the parameters of the Gamma distribution can be any positive real number, so the conjugate prior of (either parameter) of the Gamma distribution is a continuous distribution, so in particular not the Poisson distribution.
\end{solution}
\end{exercise}

\begin{exercise}
Let $X \sim \Gamm{4,2}$ and $Y \sim \Gamm{7,2}$ be independent rvs. What is the distribution of $X+Y$? What is the distribution of $\frac{X}{X+Y}$?

\begin{solution}
$X+Y \sim \text{Gamm}(11,2)$ and $\frac{X}{X+Y} \sim \text{Beta}(4,7)$.
\end{solution}
\end{exercise}





\section{Section 8.6}
\label{sec:section-8.6}

Read the definition of an order statistic. Skip the rest of BH.8.6.

\begin{exercise}
If you can answer this question, then you basically know everything you need to know about order statistics for the purpose of this course.)\\
Let $X_1,X_2,\ldots,X_9$ be a collection of random variables. Fill in the gaps (with just one word each time):
\begin{enumerate}
    \item $X_{(1)}$ denotes the ... of $X_1,X_2,\ldots,X_9$.
    \item $X_{(9)}$ denotes the ... of $X_1,X_2,\ldots,X_9$.
    \item $X_{(5)}$ denotes the ... of $X_1,X_2,\ldots,X_9$.
\end{enumerate}
\begin{solution}
\begin{enumerate}
    \item Minimum
    \item Maximum
    \item Median
\end{enumerate}
\end{solution}
\end{exercise}






\input{trailer.tex}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "study-guide.tex"
%%% End:
