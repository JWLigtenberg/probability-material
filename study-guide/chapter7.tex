% arara: pdflatex: { shell: yes }
% arara: pythontex: {verbose: yes, rerun: modified }
% arara: pdflatex: { shell: yes }
% arara: move: {  files: ['chapter7.pdf'], target: ['..'] }


\input{header}


\chapter{Chapter 7: Exercises and remarks}
\label{cha:questions-chapter-7}



\section{BH Section 7.1}


\begin{exercise}
In your own words, explain what is
\begin{enumerate}
\item a joint PMF, PDF, CDF;
\item a conditional PMF, PDF, CDF;
\item a marginal PMF, PDF, CDF.
\end{enumerate}
\begin{solution}
Check the definitions of the book.

Mistake: To say that $\P{X=x}$ is the PMF for a continuous random variable is wrong, because $\P{X=x}=0$ when $X$ is continuous.

Why is $\P{1 < x \leq 4}$ wrong notation?
hint: $X$ should be a capital.
What is the difference between $X$ and $x$?

\end{solution}
\end{exercise}

\begin{exercise}
Suppose the probability of obtaining a head twice out of two coin flips is $\P{X_1=H, X_2=H}$.
What has this to do with joint PMFs? Can you generalize this idea to other examples?
\begin{solution}
This example shows why joint distributions are important!
In any experiment that involves a sequence of measurements, such as multiple throws of a coin, or the weighing of a bunch of chimpanzees, we have to deal with joint CDFs and PMFs.
\end{solution}
\end{exercise}


\begin{exercise}
In the previous exercise, suppose the outcome of the second throw is always equal to that of the first. Specify the joint PMF.
\begin{solution}
Here, we deal with two rvs, and we have to specify how they depend. In the present case $\P{X_1= H, X_2=H} = \P{X_1=H}$ and $\P{X_1= T, X_2=T} = \P{X_1=T}$, $\P{X_1= H, X_2=T} = \P{X_1= T, X_2=H} = 0$. Note that with this, we specified the joint PMF on all possible outcomes.
\end{solution}
\end{exercise}


\begin{exercise}
We have the random vector $(X, Y) \in [0,1]^{2}$ (here $[0,1]^{2} = [0,1]\times [0,1]$) consisting of the rvs X and Y with the joint PDF $f_{X,Y}(x,y) = 2 \1{x\leq y}$.
\begin{enumerate}
\item Are $X$ and $Y$ independent?
\item Compute $F_{X,Y}(x,y)$.
\end{enumerate}
\begin{solution}
\begin{align}
f_{X}(x) &= \int_{0}^{1} f_{X,Y}(x,y) \d y = 2\int_{0}^{1} \1{x\leq y} \d y = 2\int_{x}^{1} \d y = 2(1-x) \\
f_{Y}(y) &= \int_{0}^{1} f_{X,Y}(x,y) \d x = 2\int_{0}^{1} \1{x\leq y} \d x = 2\int_{0}^{y} \d y = 2 y.
\end{align}
But $f_{X,Y}(x,y) \neq f_{X}(x)f_{Y}(y)$, hence $X,Y$ are dependent.

\begin{align}
F_{X,Y}(x,y)
&= \int_{0}^{x}\int_{0}^{y} f_{X,Y}(u,v) \d v \d u \\
&= 2\int_{0}^{x}\int_{0}^{y} \1{u\leq v} \d v \d u \\
&= 2\int_{0}^{x}\int \1{u\leq v} \1{0\leq v \leq y}\d v \d u \\
&= 2\int_{0}^{x}\int  \1{u\leq v \leq y}\d v \d u \\
&= 2\int_{0}^{x} [y-u]^{+} \d u,
\end{align}
because $u > y \implies \1{u\leq v \leq y} = 0$. Now, if $y>x$,
\begin{align}
  2\int_{0}^{x} [y-u]^{+} \d u &=
  2\int_{0}^{x} (y-u) \d u = 2 y x - x^{2},
\end{align}
while if $y\leq x$,
\begin{align}
  2\int_{0}^{x} [y-u]^{+} \d u &=
  2\int_{0}^{y} (y-u) \d u = 2 y^{2} - y^{2} = y^{2}
\end{align}

Make a drawing of the support of $f_{X,Y}$ to help to understand this better.

\end{solution}
\end{exercise}

\begin{exercise}
We have two continuous rvs $X, Y$.
Suppose the joint CDF factors into the product of the marginals, i.e., $F_{X,Y}(x,y) = F_X(x)F_Y(y)$. Can it still be possible in general that the joint PDF does not factor into a product of marginals PDFs of $X$ and $Y$, i.e., $f_{X,Y}(x,y) \neq f_X(x) f_Y(y)$?
\begin{solution}
\begin{align*}
\partial_{x}\partial_{y}F_{X,Y}(x,y)
=\partial_{x}\partial_{y}F_{X}(x) F_{Y}(y)
=\partial_{x}F_{X}(x) \partial_{y} F_{Y}(y) = f_{X}(x) f_{Y}(y).
\end{align*}
\end{solution}
\end{exercise}

\begin{exercise}
BH define the conditional CDF given an event $A$ on page 416 as $F(y|A)$.
Use this definition to write $F_{X,Y}(x,y)/F_{X}(x)$ as a conditional CDF.
Is this equal to the conditional CDF of $X$ and $Y$?
\begin{solution}
\begin{align}
\frac{F_{X,Y}(x,y)}{F_{X}(x)} = \frac{\P{X\leq x, Y\leq y}}{\P{X\leq x}}
  = \P{Y\leq y, X\leq x|X\leq x} = \P{Y\leq y|X\leq x}.
\end{align}
It is a big mistake to write $F_{X,Y}(x,y) = \P{X=x, Y=y}$. If you wrote this, recheck the definitions of BH.
\end{solution}
\end{exercise}

\begin{exercise}
Let $X$ be uniformly distributed on the set $\{0,1,2\}$ and let $Y \sim \Bern{1/4}$; $X$ and $Y$ are independent.
\begin{enumerate}
\item Present a contingency table for $X$ and $Y$.
\item What is the interpretation of the column sums of the table?
\item What is the interpretation of the row sums of the table?
\item Suppose you would change some of the entries in the table. Are $X$ and $Y$ still independent?
\end{enumerate}
\begin{solution}
$\P{X=0, Y=0} = 1/3 \cdot 3/4$,
$\P{X=0, Y=1} = 1/3 \cdot 1/4$, and so on.

If we have one column with $Y=0$ and the other with $Y=1$, then the sum over the columns are $\P{Y=0}$ and $\P{Y=1}$. The row sum for row $i$ are  $\P{X=i}$.

Changing the values will (most of the time) make $X$ and $Y$ dependent. But, what if we changes the values such that  $\P{X=0, Y=0} =1$? Are $X$ and $Y$ then again independent? Check the conditions again.
\end{solution}
\end{exercise}

\begin{exercise}
A machine makes items on a day.
Some items, independent of the other items, are failed (i.e., do not meet the quality requirements).
What are $N$ and  $p$ in the context of the chicken-egg story of BH? What are the `eggs' in this context, and what is the meaning of `hatching'?
What type of `hatching' do we have here?
\begin{solution}
  The number of produced items (laid eggs) is $N$. The probability of hatching is $p$, that is, an item is ok. The hatched eggs are the good items.
\end{solution}
\end{exercise}

% \begin{exercise}
% Apply the chicken-egg story. Families enter a zoo in a given hour. Some families have one child, other two, and so on.
% What are the `eggs' in this context, and what is the meaning of `hatching'?
% \end{exercise}


\begin{exercise}
We have two rvs $X$ and $Y$ on $\R^{+}$. It is given that $F_{X,Y}(x,y) = F_X(x)F_Y(y)$ for $x,y \leq 1/3$. It is true that then  $X$ and $Y$ are necessarily independent.
\begin{solution}
For $X, Y$ to be independent, it is necessary that  $F_{X,Y}(x,y) = F_X(x)F_Y(y)$ for all $x,y$, not just one particular choice. (This is an example that satisfying a necessary condition is not necessarily sufficient.)
\end{solution}
\end{exercise}

\begin{exercise}
I select a random guy from the street, his height $X\sim\Norm{1.8, 0.1}$, and I select a random woman from the street, her height is $Y\sim\Norm{1.7, 0.08}$.
I claim that since I selected the man and the woman independently, their heights are independent.
Briefly comment on this claim.


\begin{solution}
  Many answers are possible here, depending on extra assumptions you make.
  Here is one.
  Suppose, just by change, the fraction of taller guys in the street is a bit higher than the population fraction.
  Assuming that taller (shorter) people prefer taller (shorter) spouses, there must be a dependence between the height of the men and the women. This is because when selecting a man, I can also select his wife.

From this exercise you should memorize that \emph{independence is a property of the joint CDF, not of the rvs}.

Mistake:   $\P{Y}$ is wrong notation wrong because we can only compute the probability of an event, such as $\{Y\leq y\}$. But $Y$ itself is not an event. \end{solution}
\end{exercise}


\begin{exercise}
For any two rvs $X$ and $Y$ on $\R^{+}$ with marginals $F_{X}$ and $F_{Y}$, can  it hold that $\P{X\leq x, Y\leq y} = F_{X}(x) F_{Y}(y)$?
\begin{solution}
Only when $X, Y$ are independent.

Mistake:  independence of $X$ and $Y$ is not the same as the linear independence. Don't confuse these two types of dependene.
\end{solution}

\end{exercise}

\begin{exercise}
Theorem 7.1.11. What is the meaning of the notation $X|N=n$?
\begin{solution}
  Given $N=n$, the random variable $X$ has a certain distribution, here binomial.
\end{solution}
\end{exercise}

\begin{exercise}
Let $X, Y$ be two discrete rvs with CDF $F_{X,Y}$.  Can we compute the PDF as $\partial_{x}\partial_{y} F_{X,Y}(x,y)$?
\begin{solution}
This claim is incorrect, because $X, Y$ are discrete, hence they have a PMF, not a PDF.

Mistake: Someone said that $\partial_{x}\partial_{y}$ is not correct notation; however, it is correct! It's a (much used) abbreviation of the much heaver $\partial^{2}/\partial x \partial y$. Next, the derivative of the PMF is not well-defined (at least, not within this course. If you object, ok, but then show that you passed a decent course on measure theory.)
\end{solution}
\end{exercise}

\begin{exercise} Redo BH.7.1.24 with indicator functions and the  fundamental bridge (recall, $\P{A} = \E{\1{A}}$ for an event $A$).
(Indicators are often  easy to use, and prevent many mistakes, as is demonstrated with this example.)
\begin{solution}
\begin{align*}
\P{T_1 < T_2 } = \E{\1{T_1<T_2}} =
&= \int_0^\infty \int_0^\infty \1{t_1<t_2} f_{T_1, T_2}(t_1, t_2)\d t_{1} \d{t_2} \\
&= \int_0^\infty \int_{t_1}^{\infty}  \lambda_1 e^{-\lambda_1 t_1} \lambda_2 e^{-\lambda_2 t_2}  \d{t_2} \d{t_1}\\
&= \int_0^\infty   \lambda_1 e^{-\lambda_1 t_1} \lambda_2 \int_{t_1}^{\infty} e^{-\lambda_2 t_2}  \d{t_2} \d{t_1}\\
&= \int_0^\infty   \lambda_1 e^{-\lambda_1 t_1} e^{-\lambda_2 t_1}  \d{t_1}\\
&= \int_0^\infty   \lambda_1 e^{-\lambda_1 t_1 - \lambda_2t_{1}} \d{t_1}\\
&= \frac{\lambda_{1}}{\lambda_{1}+\lambda_2}.
\end{align*}
\end{solution}
\end{exercise}



\begin{exercise}
BH.7.1. We simulate this in one of the assignments.
\begin{hint}
Check BH 7.2.2. Bigger hint: Let $A$ the arrival time of Alice, and $B$ the time of Bob. Then we want to compute $\P{|A-B|\leq 1/4}$.  (15 minutes is 1/4 hour.)  Why is $f_{A,B}(x,y) = \1{x\in[0,1]}\1{y\in [0,1]}$? Now apply 2D-LOTUS to the function $g(x,y) = \1{|x-y|\leq 1/4}$.
\end{hint}
\begin{solution}
Use the hint. If you make a drawing, then you'll see that Alice and Bob will not meet on the triangles
$\{(x,y) : x\in [0, 3/4], y\in [1/4, 1], y>x+1/4]$ and $\{(x,y) : x\in [1/4, 1], y\in [0, 3/4], y<x-1/4]$. The area of each triangle is $(3/4)^{2}/2$, hence, the combined area is $9/16$. Therefore the probability to meet is $1-9/16=7/16$.

We can also solve   a 2D integral by first integrating along $y$, and then along $x$. Let's focus on the integral over $y$ first.
\begin{align*}
\int_0^1 \1{x<y+1/4}\1{y<x+1/4} \d y
&=\int_0^1 \1{x-1/4<y<x+1/4} \d y \\
&=\int_0^1 \1{\max\{0, x-1/4\} <y < \min\{1,x+1/4\}} \d y \\
&= \min\{1,x+1/4\} - \max\{0, x-1/4\}
\end{align*}
Now the integral over $x$:
\begin{align*}
  \int_0^1 (\min\{1,x+1/4\} - \max\{0, x-1/4\}) \d x
&=  \int_0^1 \min\{1,x+1/4\}\d x - \int_0^{1}\max\{0, x-1/4\} \d x  \\
&=  \int_0^{3/4}(x+1/4)\d x + \int_{3/4}^{1}1 \d x
 - \int_{1/4}^{1}(x-1/4) \d x \\
  &=0.5 x^2\Big|_{0}^{3/4} + 3/4\cdot 1/4 - 0.5x^2\Big|_{1/4}^1 + 3/4\cdot 1/4 \\
  &= 1/2\cdot 9/16 + 3/16 - 1/2\cdot 15/16 + 3/16 = 7/16.
\end{align*}

\end{solution}
\end{exercise}


\begin{exercise}
BH 7.9. We'll develop a simulation for this in the assignments.
\begin{hint}
a. $\P{X=i, Y=j, N=n} = \P{X=i, Y=j} \1{i+j=n}$.

c. $\P{X=i\given N=n} = 1/(n+1)$. Why is this uniform?
\end{hint}
\begin{solution}
a. The hint is the solution.

b. Use the hint for a. Then, for $k=0, 1, \ldots, n$,
\begin{equation*}
\P{X=k, N=n} = \P{X=k, Y=n-k} = pq^k p q^{n-k} = p^2 q^{n}.
\end{equation*}
(Have we used independence somewhere?)

c. Observe that the right hand side does not depend on $k$. This implies that $\P{X=k|N=n}$  also does not depend on $k$. (Why?) But, since $\P{X=k|N=n}$ is a true PMF,  is must be that $\sum_{k=0}^{n}\P{X=k|N=n}$ adds up to $1$. These two ideas put together imply that
$\P{X=k|N=n} = 1/(n+1)$.

With Bayes' expression, and using that  $\P{X=k|N=n} = 1/(n+1)$,
\begin{equation*}
\P{X=k|N=n} = \frac{\P{X=k, N=n}}{\P{N=n}},
\end{equation*}
it follows that
\begin{equation*}
\P{N=n} = \frac{\P{X=k, N=n}}{\P{X=k|N=n}} = \frac{p^2q^n}{1/(n+1)} = (n+1) p^2q^{n}.
\end{equation*}
\end{solution}
\end{exercise}

\begin{exercise} BH.7.10.

Recall that a conditional CDF given an event $A$ is defined as $F(y|A) = \P{Y\leq y|A}$. Likewise, let us write here $F_T(t|x) = \P{T\leq t|X=x}$.
Just use this in your derivation. However, there is one problem with the fact that the event $\{X=x\}$ has probablity zero. In the solution I'll discuss how to around this.

Don't forget to compare  this exercise to BH.7.9, which is the same but for discrete memoryless rvs.

\begin{solution}
Just reasoning as if there is no problem, i.e., applying Bayes' rule in a  naive way,
\begin{align*}
F_T(t|x) &= \P{T\leq t|X=x} = \P{X+Y\leq t|X=x} \\
  &=\P{Y\leq t-x, X=x}/\P{X=x} = \P{Y\leq t-x}\P{X=x}/\P{X=x}\\
  &=\P{Y\leq t-x}, 0\leq x \leq t.
\end{align*}
where I use that $Y$ and $X$ are independent to split the probability.

The problem with this derivation is that we multiply and divide by 0 ($=\P{X=x}$) just as if all is ok. But hopefully, you know that when we multiply and divide by zero, we can get any answer we like. A better way is as follows. Note beforehand that I do not expect that you could have  come up with such an answer, but you should definitely study it.


The first step is to realize that PDF $f_{T|X}(t|x)=f_{TX}(t,x)/f_X(x)$ \emph{is} well defined; we don't divide by zero because $f_{X}(x)>0$ on $x\geq 0$. By the proof of BH.8.2.1 we see that $f_{TX}(t,x) = f_X(x)f_Y(t-x) \1{0 \leq x \leq t}$, where I include the indicator to ensure that we don't run out of the support of $X$ and $T$. Thus,
\begin{align*}
  f_{T|X}(t|x) &= \frac{f_{TX}(t,x}{f_{X}(x)} = f_X(x)f_Y(t-x) \1{0 \leq x \leq t}/f_X(x) = f_Y(t-x)\1{0\leq x \leq t}.
\end{align*}
Now we know that a conditional PDF is a full-fledged PDF. So we can use idea that to \emph{define} the conditional CDF as follows:
\begin{align*}
F_{T|X}(t|x)
  := \int_0^t f_{T|X}(v|x)\1{0\leq x \leq v} \d v
    = \int_x^t \lambda e^{-\lambda(v-x)} \d v
    = \int_0^{t-x} \lambda e^{-\lambda v} \d v = 1- e^{-\lambda(t-x)}.
\end{align*}

Isn't it a bit strange that we get the same answer?
How to get out of this situation in a technically correct way is one of the hard parts of (mathematical) probability, and certainly not something we can deal with  in this course.
All books on elementary\footnote{When  in mathematics someething is elementary, it doesn't necessarily mean that that thing is simple. In fact, it can be very difficult.  Elementary means that we just dont use very advanced mathematical concepts.)} probability, and lecturers similarly, struggle with this problem; this course is not an exception, nor am I.

b.  See  part a.

c. By the above,
\begin{align*}
  f_{X|T}(x|t) &= f_{TX}(t,x)/f_T(t), \\
f_{TX}(t,x)  &=  f_X(x)f_Y(t-x) \1{0\leq x \leq t} \lambda^2 e^{-\lambda x} e^{-\lambda(t-x)} \1{0\leq x\leq t} = \lambda^2e^{-\lambda t} \1{0\leq x\leq t},\\
  &\implies  f_{X|T}(x|t) \propto \lambda^2e^{-\lambda t} \1{0\leq x\leq t} ,
\end{align*}
where the last follows because $f_T(t)$ is just a normalization constant.
Now we use some real nice, but subtle, reasoning to avoid computing $f_T$ by means of marginalizing out $x$ from $f_{T,X}(t, x)$.  Observe that $f_{TX}(t, x)$ is constant \emph{as a function of $x$} on $0\leq x \leq t$ (in other words, the RHS does not depend on $x$ on this interval). But $f_{X|T}(x|t)$  is also a real PDF. This implies  that the constant $f_T(t)$ (since it does not depend on $x$) must be  such that  $f_{X|T}(x|t)$ integrates to $1$ on $0\leq x \leq t$. The only possibility is that $f_{X|T}(x|t) = t^{-1}\1{0\leq x \leq t}$.

This reasoning gives some  offspin.  We can conclude that
\begin{align*}
  f_T(t) = f_{TX}(t, x)/f_{T|X}(t,x) = \lambda^2 t e^{-\lambda t}.
\end{align*}

This is more than a nice trick. Recall it, as it is not only used more often in the book, but also in more advanced courses on data science and machine learning.
\end{solution}
\end{exercise}


\begin{exercise}
BH.7.11
\begin{hint}
a.
First find $f_{Y|X}$ and $f{Z|X}$. Then, given $X$, $Z$ and $Y$ are iid. Hence $f_{X,Y,Z} = f_{Y, Z |X} f_{X}$. Use independence to split $f_{Y,Z|X}$ into a product.


b.
Suppose that  a realization of $Y$ is really big. Since $Y$ is dependent on $X$, $X$ must be dependent on $Y$. But $Z$ is in turn dependent on $X$. What are the consequences?

\end{hint}
\begin{solution}
a. Use the hint. Next, $f_{Y|X}(y|x) \propto e^{-(y-x)^{2}}$, and a similar expression holds for $f_{Z|X}(z|x)$. Now follow the steps of the hint.

b. Read the hint. When $Y$ is really big, $X$ must be big (with large probability), so $Z$ must be big too.

 c.
Here is the answer. The ideas are important, you'll need them during nearly any course in statistics, given the importance of the normal distribution.
\begin{align*}
f_{Y,Z}(y, z) = \int \frac 1 {2\pi} e^{-(y-x)^2/2} e^{-(z-x)^2/2} \frac 1 {\sqrt{2\pi}} e^{-x^2/2}\d x.
\end{align*}
It remains to simplify $(y-x)^2 + (z-x)^2 + x^2$. With a bit of work, it follows that this can be written as
\begin{align*}
3(x-(y+z)/3)^{2} - (y+z)^2/3+y^2+z^2.
\end{align*}
When plugging this in the integral, the last two terms appear in front of the integral. The term $(y+z)/2$ is just a shift, hence can be neglected in the integration over $x$. The $3$ has to be absorbed in the standard deviation $\sigma=1/\sqrt{3}$. And therefore,
\begin{align*}
f_{Y,Z}(y, z) = \frac 1 {2\pi} \frac 1 {\sqrt{3}} e^{-y^2/2 - z^2/2 + (y+z)^2/6}.
\end{align*}
\end{solution}
\end{exercise}

\begin{exercise}
BH.7.13
\begin{hint}
\cref{sec:memoryl-exerc-conf} contains all the explanations.
\end{hint}
\begin{solution}
Read the material of \cref{sec:memoryl-exerc-conf} for many detailed explanations on the exponential. As we will not repeat that, here are just the results.
$\P{X<Y} = 1/2$. Hence, $\P{X\leq x|X<Y} = 2\P{X\leq x, X<Y}$.
\begin{align*}
2\P{X\leq x, X<Y}
  &= 2\lambda^2\int_0^{\infty}\int_0^{\infty}   \1{u\leq x} \1{u<v} e^{-\lambda u} e^{-\lambda v} \d v \d u \\
  &= 2\lambda^2\int_0^{\infty}   \1{u\leq x} e^{-\lambda u} \int_0^{\infty} \1{u<v}  e^{-\lambda v} \d v \d u\\
  &= 2\lambda\int_0^{\infty}   \1{u\leq x} e^{-\lambda u} e^{-\lambda u}  \d u \\
  1 - e^{-2\lambda x}.
\end{align*}

b. If $X<Y$, then we know that $X=\min\{X, Y\}$. But,
\begin{align*}
  \{\min\{X,Y\}\leq x\} =  \{X\leq x, X<Y\} \cup \{Y\leq x, Y\leq X\},
\end{align*}
and the two sets on the RHS are disjoint. Hence, $\P{\min\{X,Y\}\leq x}$ is the sum of the probabilities on the RHS. By symmetry, these are equal.
\end{solution}
\end{exercise}

\begin{exercise}
BH7.15.
\begin{hint}
Make a drawing.
\end{hint}
\begin{solution}
Use the hint, that is, really make the drawing of the rectangle mentioned in the exercise. (If you refuse to to this, then nothing can  help you to understand the rest of the answer.) Then, in the drawing, note that $F(x,y)$ is the area of an (infinite) square lying  south west of the point $(x,y)$. Add and subtract such (infinite) squares until the square $[a_1,a_2] \times [b_{1},b_2]$ is covered exactly once. Realize that in the process, the square $(-\infty, a_1] \times (-\infty, b_1]$ is subtracted twice.
\end{solution}
\end{exercise}


\begin{exercise}
BH.7.24.
In the assignments we'll develop a simulator.

\begin{hint}
Check BH.7.1.24 and BH.7.1.25
First draw the area over which we have to integrate. Then use an indicator function over which to integrate. What is the joint PDF  $f_{Y_1, Y-2}$?
\end{hint}
\begin{solution}
a. From the hint,
\begin{align*}
\P{Y_1<c Y_2}
  &= \int \int \1{x<c y}\lambda_1 e^{-\lambda_1 x} \lambda_2e^{-\lambda_2 y}\d x \d y
  = \lambda_1\lambda_2\int_0^{\infty} e^{-\lambda_{1}x}\int_{x/c}^{\infty} e^{-\lambda_{2} y}\d y \d x\\
  & = \lambda_1\int_0^{\infty} e^{-\lambda_{1}x} e^{-\lambda_{2} x/c} \d x
  = \frac{\lambda_1}{\lambda_1+\lambda_2/c}.
\end{align*}
Check the result for $c=0$ and $c=\infty$.

I prefer to use conditioning, like this:
\begin{align*}
\P{Y_1<c Y_2}
  &= \int \P{Y_1<cY_2| Y_1=x}\lambda_1 e^{-\lambda_1 x} \d x
  = \int \P{Y_2>x/c| Y_1=x}\lambda_1 e^{-\lambda_1 x} \d x\\
&= \int e^{-\lambda x/c} \lambda_1 e^{-\lambda_1 x} \d x,
\end{align*}
and the rest goes as before. Actually, I tend to use conditioning as it helps to make the reasoning easier. In this case, suppose that I know that $Y_1=x$, what can I say about $\P{Y_2 > c x}$?

BTW, conditioning does not always make things simpler. When rvs are dependent, then you have to watch out.

b. See the solutions of BH on the web.
\end{solution}
\end{exercise}


\begin{exercise}
BH.7.29
\begin{solution}
All is covered in~\cref{sec:memoryl-exerc-conf}.
\end{solution}
\end{exercise}


\section{Section 7.2}
\label{sec:section-7.2}


\begin{exercise}
BH.7.2.2. Write down the integral to compute $\E{(X-Y)^{2}}$, and solve it.
\begin{solution}
We have
\begin{align}
    \E{(X-Y)^2} &= \int_{-\infty}^\infty \int_{-\infty}^\infty (x - y)^2 f_{X,Y}(x,y) \d x \d y \\
    &=\int_0^1 \int_0^1 (x - y)^2 \d x \d y \\
    &=\int_0^1 \int_0^1 (x^{2} - 2 x y +  y^2) \d x \d y \\
    &=\int_0^1 \int_0^1 x^{2} \d x \d y
    -2\int_0^1 \int_0^1  x y  \d x \d y
    +\int_0^1 \int_0^1  y^2 \d x \d y  \\
    &=\int_0^1 x^{2} \d x
    -2\int_0^1 \int_0^1 x y  \d x \d y
    + \int_0^1  y^2  \d y \\
    &=1/3  -2 \cdot 1/ 2 \cdot 1/ 2 + 1/3.
\end{align}
\end{solution}
\end{exercise}


\begin{exercise}
Explain that for a continuous r.v. $X$ with CDF $F$ and $a$ and $b$ (so it might be that $a>b$),
\begin{equation}
  \label{eq:87}
\P{a< X < b} = [F(b) - F(a)]^{+}.
\end{equation}
\begin{hint}
  Recall that $F\in [0, 1]$.
\end{hint}
\begin{solution}
\begin{align}
a<b & \implies \P{a< X < b} = F(b) - F(a) = [F(b) - F(a)]^{+} \\
a\geq b & \implies \P{a< X < b} =  0 = [F(b) - F(a)]^{+},
\end{align}
where the last equality follows from the fact that $F$ is increasing.
\end{solution}
\end{exercise}

\begin{remark}
If you are like me, you underestimate at first the importance of using indicator functions. In fact, they are extremely useful for several reasons.
\begin{enumerate}
\item  They help to keep your formulas clean.
\item You can use them in computer code as logical conditions, or to help counting relevant events, something you need when numerically estimating multi-D integrals,  for machine learning for instance.
\item  Even though figures give geometrical insight into how to integrate over an 2D area, when it comes to reversing the sequence of integration, indicators are often easier to use.
\item In fact, \emph{expectation is the fundamental concept in probability theory}, and the \emph{probability of an event is defined} as
\begin{equation}
  \label{eq:84}
  \P{A} := \E{\1{A}}.
\end{equation}
Thus, the fundamental bridge is actually an application of LOTUS to indicator functions. Hence, reread BH.4.4!
\end{enumerate}
\end{remark}

\begin{exercise}
What is $\int_{-\infty}^{\infty} \1{0\leq x \leq 3} \d x$?
\begin{solution}
\begin{align*}
\int_{-\infty}^{\infty} \1{0\leq x \leq 3} \d x =\int_{0}^{3}  \d x  = 3.
\end{align*}
\end{solution}
\end{exercise}

\begin{exercise}
What is
\begin{equation}
\label{eq:85}
\int x \1{0\leq x \leq 4} \d x?
\end{equation}
\begin{solution}
\begin{align*}
\int x \1{0\leq x \leq 4} \d x  = \int_{0}^{4} x \d x = 16/2 = 8.
\end{align*}
\end{solution}
\end{exercise}

When we do an integral over a 2D surface we can first integrate over the $x$ and then over the $y$, or the other way around, whatever is the most convenient.
(There are conditions about how to handle multi-D integral, but for this course these are irrelevant.)

\begin{exercise}
What is
\begin{equation}
\label{eq:185}
\iint x y \1{0\leq x \leq 3}\1{0\leq y \leq 4} \d x \d y?
\end{equation}
\begin{solution}
\begin{align*}
\iint xy \1{0\leq x \leq 3}\1{0\leq y \leq 4} \d x \d y
&=\int_{0}^{3} x \int_{0}^{4} y \d y \d x\\
&=\int_{0}^{3} x \frac{y^{2}} 2 \biggr|_{0}^{4} \d x\\
&= \int_{0}^{3} x\cdot 8 \d x = 8\cdot 9/2 = 4\cdot 9.
\end{align*}
\end{solution}
\end{exercise}

\begin{exercise}
What is
\begin{align}
\label{eq:285}
\iint \1{0\leq x \leq 3} \1{0\leq y \leq 4}\1{x\leq y}\d x \d y?
\end{align}
\begin{solution}
Two solutions. First we integrate over $y$.
\begin{align}
\label{eq:385}
\iint \1{0\leq x \leq 3} \1{0\leq y \leq 4}\1{x\leq y}\d x \d y
&=\int \1{0\leq x \leq 3} \int \1{0\leq y \leq 4}\1{x\leq y}\d y \d x\\
&=\int \1{0\leq x \leq 3} \int \1{\max\{x, 0\} \leq y \leq 4}\d y \d x\\
&=\int_{0}^{3} \int_{\max\{x, 0\}}^{4}\d y \d x\\
&=\int_{0}^{3} y\biggr|_{\max\{x, 0\}}^{4} \d x\\
&=\int_{0}^{3}  (4-\max\{x, 0\}) \d x\\
&=12 - \int_{0}^{3} \max\{x, 0\} \d x\\
&=12 - \int_{0}^{3} x  \d x\\
&=12 - 9/2.
\end{align}

Let's now instead first integrate over $x$.
\begin{align}
\label{eq:485}
\iint \1{0\leq x \leq 3} \1{0\leq y \leq 4}\1{x\leq y}\d x \d y
&= \int \1{0\leq y \leq 4} \int \1{0\leq x \leq 3} \1{x\leq y}\d x \d y\\
&= \int_{0}^{4} \int \1{0\leq x \leq \min\{3, y\}}\d x \d y\\
&= \int_{0}^{4} \int_{0}^{\min\{3, y\}} \d x \d y\\
&= \int_{0}^{4} \min\{3, y\}\d y\\
&= \int_{0}^{3} \min\{3, y\}\d y + \int_{3}^{4} \min\{3, y\}\d y\\
&= \int_{0}^{3} y \d y + \int_{3}^{4}  3\d y\\
&= 9/2 + 3.
\end{align}
\end{solution}
\end{exercise}


\begin{exercise}\label{ex:2a}
Take $X\sim\Unif{[1,3]}, Y\sim\Unif{[2,4]}$ and independent. Compute
\begin{equation}
  \label{eq:824}
\P{Y\leq 2X}.
\end{equation}
\begin{solution}
Take $c$ the normalization constant (why is $c=1/4$), then using the previous exercise
\begin{align}
\P{Y\leq 2X}
&=\E{\1{Y\leq 2X}} \\
&=c \int_{1}^{3}\int_{2}^{4} \1{y\leq 2x} \d y \d x \\
&=c \int_{1}^{3}\int \1{2\leq y\leq \min\{4,2x\}}  \d y \d x \\
&=c \int_{1}^{3} (\min\{4, 2x\} -2) \d x
\end{align}
Now make a drawing of the function $(\min\{4, 2x\} - 2)$ on the interval $[1,3]$ to see that
\begin{equation}
\int_{1}^{3} (min\{4, 2x\} -2) \d x = \int_{1}^{2} (2x -2) \d x + \int_{2}^{3} (4 -2) \d x.
\end{equation}
I leave the rest of the computation to you.
\end{solution}
\end{exercise}


\begin{exercise}
BH.7.38. Besides the solution of BH, read our solution.
\begin{solution}
First check~\cref{ex:3a}.

In general, I am always very careful with such `shortcuts' such as $\max\{X,Y\} + \min\{X, Y\} = X +Y$.  As a matter of fact, I try to avoid such arguments because it is easy to go wrong. Seemingly plausible arguments are often wrong due to overlooked dependency or non-linearity (effects of higher moments).

It is useful to write $\max\{x,y\} = x\1{x\geq y}+y\1{y>x}$, and something similar for the minimum. In the present case, $\cov{X,Y} = \E{X Y}-\E X \E Y$, and, similarly, $\cov{M,L} = \E{ML}- \E M \E L$, where $M$ is max, and $L$ is min. With the above indicators, it is simple to show that $\E{ML} = \E{X Y}$:
\begin{align*}
 ML
  &= (X\1{X\geq Y} + Y\1{Y\geq X})((X\1{X<Y} + Y\1{Y<X}) \\
  &= XY\1{X\geq Y} + XY\1{Y<X} = XY
\end{align*}
since $\1{X\geq Y}\1{X<Y} =0$.

However, take $X,Y\sim \Exp{\lambda}$. Then, $\E M = 3/(2\lambda)$ and $\E L= 1/(2\lambda)$, but $\E X = \E Y = 1/\lambda$.
\end{solution}
\end{exercise}

\begin{exercise}
BH.7.53. We simulate this in one of the assignments.
The ideas of this exercise find much use in finance, physics, and actuarial sciences.
In particular, the expected time it takes the drunken person---It's not only guys that sometimes consume too much alcohol---to hit some boundary is interesting. The notation of the book is a bit clumsy. Here is better notation.
Let $X_i$ be the movement along the \(x\)-axis at step $i$, and $Y_i$ along the $y$-axis.
Then $S_n=\sum_{i=1}^n X_i$ and $T_n=\sum_{j=1}^n Y_{j}$, and $R_n^2= S_n^2+T_n^2$.
\begin{hint}
Use the hint of the book and independence to see that $\E{S_{n}^2 T_n^2} = \E{S_n^{2}} \E{T_n^{2}}$.
Then try to simplify.

b. It is immediate that $\E{S_n} = 0$.
Hence, focus on $\E{S_n T_n}$. Expand  the sums of $\E{S_n T_n}$, and consider the individual terms $\E{X_i Y_j}$. When $i\neq j$, are $X_i$ and $Y_{j}$  independent? What if  $i=j$?

c. It is clear that $R_n^2=S_n^2+T_{n}^2$. Now use linearity to split $\E{R^2_n}$. Finally, realize that $\E{S_n}=0$, hence $\E{S_n^2} = \V{S_n}$. But then we can use the formula of the variance of a sum to split it up into a sum of variances plus covariances.
\end{hint}

\begin{solution}
a. In my notation, $X_i=0 \implies Y_i\neq 0$ and $X_i\neq 0 \implies Y_i=0$. The reason is that in step $i$, the drunkard makes a step left or right OR up or down. However, s/he cannot move to the right and up at the same time.

Here is an argument based on recursion. (By now I hope you see that I like this method in particular).
\begin{align*}
\E{R_n^2} = \E{(R_{n-1} + X_n + Y_n)^{2}},
\end{align*}
but $R_{n-1}$ and $X_n+Y_n$ are independent, and $\E{(X_n + Y_n)^2} = 1$. Using the recursion, $\E{R^2_n} = n$.
\end{solution}
\end{exercise}


\begin{exercise}
BH.7.58.
This is a totally great exercise. First solve it yourself. In the solution, I'll explain why, in particular how to relate the concept of covariance to the determinant of a matrix.

\begin{hint}
a. Expand the brackets in the expression for the sample variance $r$ to see that
\begin{align*}
r = 1/n \sum_i x_i y_i - \bar x \bar y.
\end{align*}
Next, we choose with probability $1/n$ one the points $(x_i, y_{i})$.  Under this probability, $\E{X Y} = 1/n \sum_i x_i y_i$, $\E X = \bar x, \E Y = \bar y$. So, how do $\cov{X,Y}$ and $r$ relate?


b. Expand the brackets and use iid and linearity properties to show that the expected area spanned by two random points $(X,Y)$ and $(\tilde X, \tilde Y)$ satisfies
\begin{align*}
\E{(X-\tilde X)(Y-\tilde Y)} = 2\cov{X,Y}.
\end{align*}

\end{hint}
\begin{solution}

b. Use the hint. Then, if we choose two points at random from the sample, then $(x_i-x_j)(y_i-y_j)$ is the area spanned by these  two points.
More generally, I have $n$ choices for my first point, and also $n$ choices for the second point (if both points are the same, the area of the rectangle is 0, so we don't have to exclude such choices).
Hence, the expected area of the rectangle spanned by the two random points $(X,Y)$ and $(\tilde X, \tilde Y)$ is
\begin{align*}
\frac 1 {n^2} \sum_{i,j} (x_i-x_j)(y_i-y_j).
\end{align*}
Simplify this to show that
\begin{align*}
2 \frac 1 n \sum_i x_i y_i - 2 \bar x \bar y = 2 r
\end{align*}
Hence, by part a., the expected area is twice the covariance.

Why is $\cov{X,a}= 0$ for $a$ a constant? Because the `area' of rectangles, all with the same \(y\)-coordinate, is zero, i.e., they lie on a line.

c.  This is the part of the exercise that explains what the above is all about.
Since there is a direct relation between covariance and area, we can use geometric arguments to derive (and memorize!) all properties of covariance! Write property i. of covariance  as $\cov{X,Y} = \cov{Y,X}$. Suppose I flip the \(x\) and \(y\)-axis, does the area of a rectangle change?  For property ii., what happens to the area of rectangle if you stretch the sides? For property iii., realize that this is just a shift of a rectangle that leaves its area invariant. For property iv., what happens to the area if you put an extra rectangle on top or to the right?

BTW, property iii. follows directly from property iv. In iv., take $W_3$ equal to a constant $a_2$, in other words $\P{W_3=a_2}=1$. We know that $\cov{X, a} = 0$ for a constant $a$.

Here are  some final remarks.

Let's put all the above in a very general frame.  The covariance has a number of interesting properties:
\begin{enumerate}
\item  It is bilinear, that is, the covariance is linear in both arguments. The linearity in the first argument means that $\cov{X+Y, Z}=\cov{X,Z}+\cov{Y,Z}$ and $\cov{a X, Z}=a\cov{X,Z}$ for $a\in \R$. The linearity in the second argument means that $\cov{X, Y+Z}=\cov{X,Y}+\cov{X,Z}$ and $\cov{X, a Z}=a\cov{X,Z}$ for $a\in \R$.
\item It is symmetric: $\cov{X, Y}=\cov{Y,X}$, from which we define $\V X = \cov{X,X}$.
\item  $\cov{X,a} = 0$ for all $a\in R$.
\end{enumerate}
If you memorize the first two properties of covariance, all the rest follows.

Now we do some geometry. Take three vectors $x,y, z\in \R^2$ (it's easy to generalize to $\R^n)$. Then we know that the area $D(x,y)$ of the parallelogram spanned by vectors $x$ and $y$  satisfies the following properties.
\begin{enumerate}
\item  Area is bilinear. The linearity in the first argument means that $D(x+y, z) = D(x, z) + D(y, z)$ and $D(ax, z)=a D(x, z)$ for $a\in \R$. (Just make a drawing to convince you about this.) The linearity in the second argument means that  $D(x, y+z) = D(x, y) + D(x, z)$  and $D(x, a z)=a D(x, z)$ for $a\in \R$.
\item  $D(x,x)$ = 0; there is no area between $x$ and $x$.
\item $D( (1,0), (0,1)) = 1$; the area of the square with side 1 is 1.
\end{enumerate}
In fact, thex first property means that stretching vectors and stacking parallelograms result in stretching and adding areas.
The second says that the area of a parallelogram spanned by two parallel vectors is zero. The third specifies that the area of the unit square is 1.

Now it can be proven that there exists just one function $D$ that satisfies these properties. In fact, this is the determinant of the matrix with as columns the vectors that span the parallellogram. Moreover, it can be shown that the second property can be replaced by the skew-symmetric property: $D(x,y) = - D(x,y)$.
(Note that $D(x,x) = -D(x,x) \implies 2 D(x,x) = 0 \implies D(x,x) = 0$.)

Let us use the properties to compute the area of a parallelogram spanned by the vectors $x = (a,b)$ and $y =(c,d)$ in 2D. Then
\begin{align*}
D(x,y) &= D((a,b), (c,d)) =  D(a (1,0)) + b(0,1), c(1,0) + d(0,1)) \\
&= ad D((1,0), (0,1)) + b c D((0,1), (1,0)) = ad - b c,
\end{align*}
where we use bilinearity in the first step, and skew-symmetry in the second and third. And this is indeed the determinant of the matrix with $x$ and $y$ as columns.

So, all in all, this is what I remembered throughout the years: the covariance and the determinant are bi-linear forms, the first is symmetric, the second skew- (or anti-)symmetric.

Finally, I don't see why the areas of the rectangles have to have a sign in this problem. Interestingly, for the determinant, the areas of the parallelograms do have to have a sign to make the concept useful for physics.
\end{solution}
\end{exercise}

\begin{exercise}
BH.7.59.
Read this exercise, then read (and do) BH.5.53 for some further background.
You'll encounter these topics countless times in other courses!
The final answer is really nice and intuitive.

\begin{hint}
a. Use that expectation is linear.

b. Read the entire exercise in its entirety before trying to solve it. In this case trying to solve c.\/ seems simpler because of the extra iid assumption. You  might want to use this to formulate some simple guesses.

Thus first part c. It is given that the $X_i$ and $Y_j$ are iid. Then, if I could improve the estimator $\hat \theta$ by splitting the measurements into two sets $X_i$ and $Y_j$, then I would certainly do that.
And not only I would do that; anybody in his right mind would do that.
But, I never heard of this idea, and I am sure you have neither, so this must be impossible (because if it would, people would have been using this trick for ages.)
Hence, we can place this in the context of the maxim: `we cannot obtain information for free'.
For this case, this must imply that splitting iid measurements into smaller sets cannot help with improving the estimator. What does this idea imply for the weights?


Part b, continued. I always try to solve the problem myself without a hint. This lead to the following considerations, which gave me quite a bit of extra understanding beyond the problem itself.  As a next piece of advice, before doing hard work, I prefer to look at some corner cases to acquire some intuitive understanding. I also use the rvs of Part c.

Suppose  that $v_2:=\V{Y_j} = 0$, but $v_1 := \V{X_i} > 0$. (For instance, $Y_j$ is the $j$th measurement of a perfect machine and $X_j$ of an imperfect machine.)
Then we know that the set $\{Y_j\}$ forms a set of perfect measurements.
But then I am not interested in the $\{X_i\}$ measurements anymore; why should I as I have the perfect measurements $\{Y_j\}$ at my disposal.
So, then I put $w_1=0$, because I don't want the $\{X_i\}$ measurements to pollute my estimator.
In other words, the final result should be such that $v_2=0 \implies w_{1}=0$, and vice versa.


More generally, I learned from this  corner case that I want this for the final result:  when $v_2<v_{1} \implies w_1 < w_{2}$, and vice versa.

How would you choose the weights such that this requirement is satisfied, but also the condition imposed by Part c.?
\end{hint}


\begin{solution}
a. Follows directly from the hint.

Check the hint!

c.  If $X_i$ and $Y_j$ are iid, it must be that $w_{1} = n/(n+m)$.

b. Can we make some further progress, just by keeping a clear mind?
Well, in fact we can by using our insights of part c.
If we have $n+m$ iid measurements of which we call $n$ measurements of type $X_i$, and $m$ of type $Y_{j}$, then
\begin{align*}
\V{\hat \theta_{1}}  =  \E{\left(\frac{1}{n}\sum_{i}X_{i} - \theta\right)^2} = n^{-2}\E{\left(\sum_i (X_{i}-\theta)\right)^{2}} = n^{-2}\V{\sum_i X_i} = \V{X_1}/n = \sigma^{2}/n.
\end{align*}
So, $n=\sigma^{2}/\V{\hat \theta_1}$, and likewise $m=\sigma^{2}/\V{\hat \theta_2}$. Finally, plug this into our earlier expression for $w_1$ to  get
\begin{align*}
w_1 = \frac n {n+m} = \frac{\sigma^2/\V{\hat \theta_1}}{\sigma^2/\V{\hat \theta_1} + \sigma^2/\V{\theta_2}} = \frac{\V{\hat \theta_2}}{\V{\hat \theta_1} + \V{\theta_2}}.
\end{align*}
If we check our earlier insight, then we see that if $\V{Y_j}=0$, then $\V{\theta_2}=0$, hence $w_1=0$ in that case. This is precisely what we wanted.

Let us finally use the  hint of BH to check that the above expression for $w_1$ is correct.
\begin{align*}
\E{(\hat \theta -\theta)^{2}} =
\E{(w_1(\hat \theta_{1} - \theta) + w_2(\hat \theta_{2}-\theta))^{2}} =
\V{w_1 \hat \theta_{1}} + \V{w_2 \hat \theta_{2}},
\end{align*}
by independence. Take the $w$'s out of the variances, then write $w_2=1-w_1$, take $\partial_{w_1}$ of the expression,  set the result to 0, and solve for $w_1$. You'll get the above expression.
\end{solution}
\end{exercise}

\begin{exercise}
BH.7.86. The concepts discussed here are a standard part of the education of GPs (i.e., medical doctors), and in data science in general.
\begin{hint}
The challenge for you is to try to understand the mathematics behind these concepts.
Read the exercise a number of times. I found it quite difficult to capture the concepts in formulas. (I solved it once. After two weeks,  I tried to solve it again, and found it just as hard as the first time.) Once you have the model, the technical part itself is simple.
\end{hint}
\begin{solution}
a. It is given that $\P{T\leq t\given D=1} = G(t)$ and $\P{T\leq t\given D=0} = H(t)$. From Theorem 5.3.1.i,  we have that we can associate a rv. to a CDF F. Sometimes we say that the CDF $F$ /induces/ a rv. $X$.  So let us use this here to say that $G$ induces the rv. $T_1$ and $H$ induces $T_0$.
So the /sensitivity/ is $\P{T_1>t_0} = 1-G(t_0)$ and the /specificity/ is $\P{T_1<t_0} = H(t_0)$.

To make the ROC plot, I first made two plots, one of the sensitivity and the other for 1 minus the specificity, i.e., $1-H(t_0)$.
Then, in the ROC plot, we put a specificity of $s$ on the \(x\)-axis, then we search for a $t$ such that $1-H(t) = s$, and then we plug this $t$ into $1-G(t)$ to get the sensitivity.
To help you understand this better, check that $s=0 \implies t = b \implies 1-G(t) = 0$.
Moreover, check that $s=1\implies t=a \implies 1- G(t) = 1$.
Hence, the ROC curve starts in the origin and stops at the point $(1,1)$.

With this insight, the area under the ROC curve can be written as
\begin{align*}
\int_0^1 (1-G(H^{-1}(1-s))) \d s  =
1 - \int_0^1 G(H^{-1}(1-s)) \d s  =
1 - \int_a^b G(t) h(t) \d t,
\end{align*}
where, in the last step, we use the 1D change of variable $H(t)=1-s \implies h(t) \d t = -\d s$. It remains to  interpret the integral, so let's plug in the definitions:
\begin{align*}
\int_a^b G(t) h(t) \d t =
\int_a^b \P{T_1\leq t} f_{T_0}(t) \d t =
\int_a^b \P{T_1\leq T_0\given T_0 = t} f_{T_0}(t) \d t =  \P{T_{1\leq T_0}}.
\end{align*}

\end{solution}
\end{exercise}



\section{Section 7.3}
\label{sec:section-7.3}



\begin{exercise}
Give a brief example of a situation where it might be more convenient to employ the correlation than the covariance.  Explain why.
\begin{solution}
The covariance might be a large number, which may  suggest that the rvs $X$ and $Y$ are `very' dependent. However, when $\V X$ and $\V Y$ are also large, the correlation can be small. Thus, correlation is a scaled type of covariance.
\end{solution}
\end{exercise}

\begin{exercise}
In queueing theory  the concept of squared coefficient of variance $SCV$ of a rv $X$ is very important. It is defined as $C = \V{X}/(\E X)^{2}$. Is the SCV of $X$ equal to $\text{Corr}(X,X)$? Can it happen that $C>1$?
\begin{solution}
Answers: no and yes.

We have
\begin{align}
    C = \frac{\V{X}}{(\E{X})^2},
\end{align}
which does not equal
\begin{align}
    \text{Corr}(X,X) = \frac{\cov{X,X}}{\sqrt{\V{X}\V{X}}} = 1
\end{align}
in general (for instance, consider a degenerate random variable $X \equiv 1$). Next, consider a $N(1,100)$ random variable. Then,
\begin{align}
    C = 100/(1^2) = 100 > 1.
\end{align}
\end{solution}
\end{exercise}


\begin{exercise}
Prove the key properties 1--5 of the covariance below BH.7.3.2.
\begin{solution}
\begin{enumerate}
    \item We have
    \begin{align}
        \cov{X,X} = \E{XX} - \E{X}\E{X} = \E{X^2} - \E{X}^2 = \V{X}.
    \end{align}
    \item We have
    \begin{align}
        \cov{X,Y} = \E{XY} - \E{X}\E{Y} = \E{YX} - \E{Y} \E{X} = \cov{Y, X}.
    \end{align}
    \item We have
    \begin{align}
        \cov{X,c} = \E{Xc} - \E{X}\E{c} = c\E{X} - c\E{X} = 0.
    \end{align}
    \item We have
    \begin{align}
        \cov{aX,Y} = \E{aXY} - \E{aX}\E{Y} = a\big(\E{XY} - \E{X}\E{Y}\big) = a\cov{X,Y}.
    \end{align}
    \item We have
    \begin{align}
        \cov{X+Y, Z} &= \E{(X + Y)Z} - \E{X+Y}\E{Z} \\
        &= \E{XZ + YZ} - \big(\E{X} + \E{Y}\big) \E{Z}\\
        &= \E{XZ} - \E{X}\E{Z} + \E{YZ} - \E{Y}\E{Z} \\
        &= \cov{X,Z} + \cov{Y,Z}.
    \end{align}
\end{enumerate}
\end{solution}
\end{exercise}

\begin{exercise}
Using the definition of Covariance (BH.7.3.1) derive the expression $\cov{X,Y}=\E{XY}-\E{X}\E{Y}$. Use this to show why independence of X and Y implies their uncorrelatedness (Note that the converse does not hold).
\begin{solution}
We have
\begin{align}
    \cov{X,Y} &= \E{(X - \E{X})(Y - \E{Y})} \\
    &= \E{XY - X\E{Y} - Y\E{X} + \E{X}\E{Y}} \\
    &= \E{XY} - \E{X}\E{Y} - \E{Y} \E{X} + \E{X}\E{Y}\\
    &= \E{XY}-\E{X}\E{Y}.
\end{align}
When $X$ and $Y$ are independent, then $\E{XY} = \E X \E Y$, and then $\cov{X,Y}=0$.
\end{solution}
\end{exercise}

\begin{exercise}
Let $U, V$ be two rvs and let $a,b\in \R$.
Use the previous question to express $\cov{a(U+V), b(U-V)}$ in terms of $\V{U}$, $\V{V}$ and $\cov{U,V}$.
\begin{solution}
By linearity of the covariance wea have
\begin{align}
    \cov{a(U+V), b(U-V)} &= a \Big( \cov{U, b(U-V)} + \cov{V, b(U-V)} \Big) \\
    &= a \Big( b\big( \cov{U, U} - \cov{U, V} \big)  + b\big( \cov{V, U} - \cov{V,V} \Big) \\
    &= a \Big( b\big( \cov{U, U} - \cov{U, V} \big)  + b\big( \cov{V, U} - \cov{V,V} \Big) \\
    &= a b \Big( \V{U} - \cov{U, V} + \cov{V, U} - \V{V} \Big) \\
    &= a b \Big( \V{U} - \V{V} \Big).
\end{align}
Alternatively one can also use the result from BH.7.1.26, according to which $\cov{X,Y} = \E{XY}-\E{X}\E{Y}$.
\end{solution}
\end{exercise}


\begin{exercise}\label{ex:3a}
The solution of BH.7.3.6 is a somewhat tricky; I would have not  found this trick myself. Here is an approach that is trick free.

Neglecting the event $\{X=Y\}$ as this has zero probability, we know that $M=X, L=Y$ or $M=Y, L=X$. Use this idea and the formula $\cov{M,L} = \E{ML} - \E M \E L$ to derive the result of this example.
\begin{hint}
Realize that $\E{ML} = \E{XY}$.
\end{hint}
\begin{solution}
With the hint: $\E{XY}=1/\lambda^{2}$, when $X, Y \sim\Exp{\lambda}$. Then, $L\sim \Exp{2\lambda}$, since $f_{L}(x) = 2 f_X(x) (1-F_Y(x)) = 2 \lambda e^{-2\lambda x}$. Therefore, $\E L = 1/2\lambda$. Also, by memoryless, $\E{M} = \E L + \E X = 3/2\lambda$. Hence, $\E{M}\E L = 3/4\lambda^{2}$. Hence, $\E{ML}- \E M \E L = 1/\lambda^{2-} 3/4\lambda^2 = 1/4\lambda^{2}$.
\end{solution}
\end{exercise}



\section{Section 7.4}
\label{sec:section-7.4}


\begin{exercise}
Come up with a short illustrative example in which the random vector $\mathbf{X} = (X_1, \ldots, X_6)$ follows a Multinomial Distribution with parameters  $n=10$ and $\mathbf{p}=(\frac{1}{6}, ..., \frac{1}{6}) \in \R^{6}$.
\begin{solution}
We throw 10 fair dice. $X_i$ denotes the number of dice that show the number $i$, $i=1,\ldots,6$.
\end{solution}
\end{exercise}


\begin{exercise}
BH.7.71.

\begin{hint}

b. The people in the sample of size $n$ with an $A$ is $X_1+X_2$. But this is the same as $n-X_3$. Hence, what is $\P{X_3=n-i}$?


c. I found this a hard problem.
Here is my hint based on recursion.
Let $S_n$ be the number of $A$s in $n$ individuals.
We want to know $f_n(i) = \P{S_n=i}$.
A simple recursive idea, i.e., one-step analysis by conditioning on the phenotype of the $n$th person, gives that
\begin{align*}
f_n(i)=f_{n-1}(i-2) p^2 + f_{n-1}(i-1) 2p q + f_{n-1}(i)q^2,
\end{align*}
with $q=1-p$ as always. Now I was a bit stuck, but just to try to see whether I could see some structure, I tried a simpler case, namely, a recursion for the binomial distribution. Derive this, and then use this to solve the problem.


d. It is easiest to work with $f(p) = \log \P{X_1=k, X_2=l, X_3 = m}$, where $\P{X_1=k, X_2=l, X_3 = m}$ follows from a., and then differentiate with respect to $p$.

e. Follow the same scheme as for d.
\end{hint}

\begin{solution}
a. Multinomial.

b. With the hint we end up at $X_1+X_2\sim \Bin{n, p^2+2p(1-p)}$.

c. Here is a short intermezzo on finding a recursion for the sum of a number of Bernouilli rvs.  Let $S_n$ be the number of successes in the binomial, and write $g_n(i) = \P{S_n=i}$ for this case.
Then,
\begin{align*}
g_n(i)&= g_{n-1}(i-1)p +  g_{n-1}(i)q \\
&= (g_{n-2}(i-2)p + g_{n-2}(i-1)q)p + (g_{n-2}(i-1)p+g_{n-2}(i)q)q \\
&= g_{n-2}(i-2)p^2 + g_{n-2}(i-1)2p q + g_{n-2}(i)q^{2}.
\end{align*}
I also know that $g_n(i) = {n \choose i} p^iq^{n-i}$.
End of intermezzo.

Now compare the recursion with $f_n(i)$ for the genes tox the expression for the binomial.
They are nearly the same, except that in the genes case, the `n' seems to run twice as fast.
I then tried the guess $f_n(i) = {2n \choose i} p^i q^{2n-i}$.
For you, plug it in, and show that it works.

So, what was my overall approach?
I used recursion, but got stuck.
Then I used recursion for a simpler case whose solution I know by heart.
I compared the recursions for both cases to see whether I could recognize a pattern.
This lead me to a guess, which I verified by plugging it in.
Using recursion is not guaranteed to work, of course, but often it's worth a try.

Now, looking back, I realize that it is as if individual $n$ adds the outcome of two coin flips (with values in $AA$, $Aa$ or $a a$) to the sum $S_{n}$ of $A'$s. For you to solve: what is the distribution of two coin flips? Next, $S_n$ is just the sum of $n$ individual `double coin flips'. Hence, what must the distribution of $S_n$ be?

d. It is easiest to work with $f(p) = \log \P{X_1=k, X_2=l, X_3 = m}$. With part a. this can be written as
\begin{align*}
f(p) = C + (2k+l)\log p + (l+2m)\log(1-p),
\end{align*}
where $C$ is a constant (the log of the normalization constant). (BTW, with this you can check your answer for part a.)
Compute $\d f(p)/\d p = 0$, because at this $p$, $\log f$, hence $f$ itself, is maximal.  Observe that $C$ drops out of the computation, because when differentiating, it disappears.


e. Now we like to know what $p$  maximizes $\P{X_3=n-i}$. Take $g(q) = \log \P{X_3 = n-i}$, then
\begin{align*}
g(q) = C + i \log (1-q^{2}) + 2 (n-i)\log q.
\end{align*}
(With this, check your answer of part b.) Again, take the derivative (with respect to $q$), and solve for $q$.
\end{solution}
\end{exercise}



\section{Section 7.5}



\begin{exercise}
Is the following claim correct? If the rvs $X, Y$ are both normally distributed, then $(X, Y)$ follows a Bivariate Normal distribution.
\begin{solution}
No, this does not always hold, see BH.7.5.2. However, it does hold when $X$ and $Y$ are independent.
\end{solution}
\end{exercise}
\begin{exercise}
Let $X, Y, Z$ be iid $\mathcal{N}(0,1).$ Determine whether or not the random vector
\begin{align*}
    \mathbf{W} = (X+2Y, 3X+4Z, 5Y+6Z, 2X-4Y+Z, X-9Z, 12X+\sqrt{3}Y -\pi Z + 18)
\end{align*}
is Multivariate Normal. (Explain in words, don't do a lot of tedious math here!)
\begin{solution}
Since $X,Y,Z$ are independent normally distributed variables, $(X,Y,Z)$ is multivariate normally distributed. Hence, every linear combination of $X,Y,Z$ is normally distributed. Note that every linear combination of the elements of $W$ can be written as a linear combination of $X,Y,Z$. Hence, every linear combination of the elements of $W$ is normally distributed. Hence, $W$ is multivariate normally distributed.
\end{solution}
\end{exercise}



\input{trailer}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "study-guide.tex"
%%% End:
