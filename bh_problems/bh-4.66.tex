\input{header.tex}
\setcounter{theorem}{65}
\begin{exercise} [BH.4.66]
%\begin{hint}
%\end{hint}
\begin{solution}
    \begin{enumerate}
	    \item The estimator is biased:
    	\begin{align*}
    		\mathbb{E}T &= \sum_{i=0}^{\infty} e^{-3i} e^{-\lambda}\frac{\lambda^i}{i!}= e^{-\lambda} \sum_{i=0}^{\infty}  \frac{\left(\lambda e^{-3} \right)^i}{i!} \\
    		&= e^{-\lambda} e^{\lambda/e^3} \neq \theta (=e^{-3\lambda})
    	\end{align*}
    	\item Is it unbiased:
    	\begin{align*}
    		\mathbb{E}(-2)^X  &= \sum_{i=0}^{\infty} {-2}^i e^{-\lambda}\frac{\lambda^i}{i!}= e^{-\lambda} \sum_{i=0}^{\infty}  \frac{\left(-2\lambda \right)^i}{i!} \\
    		&= e^{-\lambda} e^{-2\lambda} = \theta 
    	\end{align*} 
    	\item (Solution from the online solution manual by Joseph K. Blitzstein and Jessica Hwang) \textit{The estimator $g(X)$ is silly in the sense that it is sometimes negative, whereas $e^{-\lambda}$
    		is positive. One simple way to get a better estimator is to modify $g(X)$ to make it nonnegative, by letting $h(X)$ = 0 if $g(X)$ $<$ 0 and $h(X)$ = $g(X)$ otherwise.
    		Better yet, note that $e^{-\lambda}$ is between 0 and 1 since $\lambda$ $>$ 0, so letting $h(X)$ = 0 if
    		$g(X)$ $<$ 0 and $h(X)$ = 1 if $g(X)$ $>$ 0 is clearly more sensible than using $g(X)$.} \\~\\
    	Notice here the estimator is based on one r.v., which is slightly different in common statistical setup where we usually assume $n$ observations. This case is more like one scenario where you have to make one educated guess based on one observation (something like look out though the window and say what the weather is like). 
	\end{enumerate}
\end{solution}
\end{exercise}
\input{trailer.tex}
