% arara: pdflatex: { shell: yes }
% arara: pythontex: {verbose: yes, rerun: modified }
% arara: pdflatex: { shell: yes }
% arara: move: {  files: ['chapter7.pdf'], target: ['..'] }


\input{header}


\setcounter{chapter}{6}
\chapter{Chapter 7}
\label{cha:questions-chapter-7}



\subfile{bh-7.1.tex}
\subfile{bh-7.9.tex}
\subfile{bh-7.10.tex}


\setcounter{theorem}{10}
\begin{exercise}
BH.7.11
\begin{hint}
a.
First find $f_{Y|X}$ and $f{Z|X}$. Then, given $X$, $Z$ and $Y$ are iid. Hence $f_{X,Y,Z} = f_{Y, Z |X} f_{X}$. Use independence to split $f_{Y,Z|X}$ into a product.


b.
Suppose that  a realization of $Y$ is really big. Since $Y$ is dependent on $X$, $X$ must be dependent on $Y$. But $Z$ is in turn dependent on $X$. What are the consequences?

\end{hint}
\begin{solution}
a. Use the hint. Next, $f_{Y|X}(y|x) \propto e^{-(y-x)^{2}}$, and a similar expression holds for $f_{Z|X}(z|x)$. Now follow the steps of the hint.

b. Read the hint. When $Y$ is really big, $X$ must be big (with large probability), so $Z$ must be big too.

 c.
Here is the answer. The ideas are important, you'll need them during nearly any course in statistics, given the importance of the normal distribution.
\begin{align*}
f_{Y,Z}(y, z) = \int \frac 1 {2\pi} e^{-(y-x)^2/2} e^{-(z-x)^2/2} \frac 1 {\sqrt{2\pi}} e^{-x^2/2}\d x.
\end{align*}
It remains to simplify $(y-x)^2 + (z-x)^2 + x^2$. With a bit of work, it follows that this can be written as
\begin{align*}
3(x-(y+z)/3)^{2} - (y+z)^2/3+y^2+z^2.
\end{align*}
When plugging this in the integral, the last two terms appear in front of the integral. The term $(y+z)/2$ is just a shift, hence can be neglected in the integration over $x$. The $3$ has to be absorbed in the standard deviation $\sigma=1/\sqrt{3}$. And therefore,
\begin{align*}
f_{Y,Z}(y, z) = \frac 1 {2\pi} \frac 1 {\sqrt{3}} e^{-y^2/2 - z^2/2 + (y+z)^2/6}.
\end{align*}
\end{solution}
\end{exercise}

\setcounter{theorem}{12}
\begin{exercise}
BH.7.13
\begin{hint}
\cref{sec:memoryl-exerc-conf} contains all the explanations.
\end{hint}
\begin{solution}
Read the material of \cref{sec:memoryl-exerc-conf} for many detailed explanations on the exponential. As we will not repeat that, here are just the results.
$\P{X<Y} = 1/2$. Hence, $\P{X\leq x|X<Y} = 2\P{X\leq x, X<Y}$.
\begin{align*}
2\P{X\leq x, X<Y}
  &= 2\lambda^2\int_0^{\infty}\int_0^{\infty}   \1{u\leq x} \1{u<v} e^{-\lambda u} e^{-\lambda v} \d v \d u \\
  &= 2\lambda^2\int_0^{\infty}   \1{u\leq x} e^{-\lambda u} \int_0^{\infty} \1{u<v}  e^{-\lambda v} \d v \d u\\
  &= 2\lambda\int_0^{\infty}   \1{u\leq x} e^{-\lambda u} e^{-\lambda u}  \d u \\
  1 - e^{-2\lambda x}.
\end{align*}

b. If $X<Y$, then we know that $X=\min\{X, Y\}$. But,
\begin{align*}
  \{\min\{X,Y\}\leq x\} =  \{X\leq x, X<Y\} \cup \{Y\leq x, Y\leq X\},
\end{align*}
and the two sets on the RHS are disjoint. Hence, $\P{\min\{X,Y\}\leq x}$ is the sum of the probabilities on the RHS. By symmetry, these are equal.
\end{solution}
\end{exercise}

\setcounter{theorem}{14}
\begin{exercise}
BH7.15.
\begin{hint}
Make a drawing.
\end{hint}
\begin{solution}
Use the hint, that is, really make the drawing of the rectangle mentioned in the exercise. (If you refuse to to this, then nothing can  help you to understand the rest of the answer.) Then, in the drawing, note that $F(x,y)$ is the area of an (infinite) square lying  south west of the point $(x,y)$. Add and subtract such (infinite) squares until the square $[a_1,a_2] \times [b_{1},b_2]$ is covered exactly once. Realize that in the process, the square $(-\infty, a_1] \times (-\infty, b_1]$ is subtracted twice.
\end{solution}
\end{exercise}


\setcounter{theorem}{23}
\begin{exercise}
BH.7.24.
In the assignments we'll develop a simulator.

\begin{hint}
Check BH.7.1.24 and BH.7.1.25
First draw the area over which we have to integrate. Then use an indicator function over which to integrate. What is the joint PDF  $f_{Y_1, Y-2}$?
\end{hint}
\begin{solution}
a. From the hint,
\begin{align*}
\P{Y_1<c Y_2}
  &= \int \int \1{x<c y}\lambda_1 e^{-\lambda_1 x} \lambda_2e^{-\lambda_2 y}\d x \d y
  = \lambda_1\lambda_2\int_0^{\infty} e^{-\lambda_{1}x}\int_{x/c}^{\infty} e^{-\lambda_{2} y}\d y \d x\\
  & = \lambda_1\int_0^{\infty} e^{-\lambda_{1}x} e^{-\lambda_{2} x/c} \d x
  = \frac{\lambda_1}{\lambda_1+\lambda_2/c}.
\end{align*}
Check the result for $c=0$ and $c=\infty$.

I prefer to use conditioning, like this:
\begin{align*}
\P{Y_1<c Y_2}
  &= \int \P{Y_1<cY_2| Y_1=x}\lambda_1 e^{-\lambda_1 x} \d x
  = \int \P{Y_2>x/c| Y_1=x}\lambda_1 e^{-\lambda_1 x} \d x\\
&= \int e^{-\lambda x/c} \lambda_1 e^{-\lambda_1 x} \d x,
\end{align*}
and the rest goes as before. Actually, I tend to use conditioning as it helps to make the reasoning easier. In this case, suppose that I know that $Y_1=x$, what can I say about $\P{Y_2 > c x}$?

BTW, conditioning does not always make things simpler. When rvs are dependent, then you have to watch out.

b. See the solutions of BH on the web.
\end{solution}
\end{exercise}


\setcounter{theorem}{28}
\begin{exercise}
BH.7.29
\begin{solution}
All is covered in~\cref{sec:memoryl-exerc-conf}.
\end{solution}
\end{exercise}

\setcounter{theorem}{37}
\begin{exercise}
BH.7.38. Besides the solution of BH, read our solution.
\begin{solution}
First check~\cref{ex:3a}.

In general, I am always very careful with such `shortcuts' such as $\max\{X,Y\} + \min\{X, Y\} = X +Y$.  As a matter of fact, I try to avoid such arguments because it is easy to go wrong. Seemingly plausible arguments are often wrong due to overlooked dependency or non-linearity (effects of higher moments).

It is useful to write $\max\{x,y\} = x\1{x\geq y}+y\1{y>x}$, and something similar for the minimum. In the present case, $\cov{X,Y} = \E{X Y}-\E X \E Y$, and, similarly, $\cov{M,L} = \E{ML}- \E M \E L$, where $M$ is max, and $L$ is min. With the above indicators, it is simple to show that $\E{ML} = \E{X Y}$:
\begin{align*}
 ML
  &= (X\1{X\geq Y} + Y\1{Y\geq X})((X\1{X<Y} + Y\1{Y<X}) \\
  &= XY\1{X\geq Y} + XY\1{Y<X} = XY
\end{align*}
since $\1{X\geq Y}\1{X<Y} =0$.

However, take $X,Y\sim \Exp{\lambda}$. Then, $\E M = 3/(2\lambda)$ and $\E L= 1/(2\lambda)$, but $\E X = \E Y = 1/\lambda$.
\end{solution}
\end{exercise}

\setcounter{theorem}{52}
\begin{exercise}
BH.7.53. We simulate this in one of the assignments.
The ideas of this exercise find much use in finance, physics, and actuarial sciences.
In particular, the expected time it takes the drunken person---It's not only guys that sometimes consume too much alcohol---to hit some boundary is interesting. The notation of the book is a bit clumsy. Here is better notation.
Let $X_i$ be the movement along the \(x\)-axis at step $i$, and $Y_i$ along the $y$-axis.
Then $S_n=\sum_{i=1}^n X_i$ and $T_n=\sum_{j=1}^n Y_{j}$, and $R_n^2= S_n^2+T_n^2$.
\begin{hint}
Use the hint of the book and independence to see that $\E{S_{n}^2 T_n^2} = \E{S_n^{2}} \E{T_n^{2}}$.
Then try to simplify.

b. It is immediate that $\E{S_n} = 0$.
Hence, focus on $\E{S_n T_n}$. Expand  the sums of $\E{S_n T_n}$, and consider the individual terms $\E{X_i Y_j}$. When $i\neq j$, are $X_i$ and $Y_{j}$  independent? What if  $i=j$?

c. It is clear that $R_n^2=S_n^2+T_{n}^2$. Now use linearity to split $\E{R^2_n}$. Finally, realize that $\E{S_n}=0$, hence $\E{S_n^2} = \V{S_n}$. But then we can use the formula of the variance of a sum to split it up into a sum of variances plus covariances.
\end{hint}

\begin{solution}
a. In my notation, $X_i=0 \implies Y_i\neq 0$ and $X_i\neq 0 \implies Y_i=0$. The reason is that in step $i$, the drunkard makes a step left or right OR up or down. However, s/he cannot move to the right and up at the same time.

Here is an argument based on recursion. (By now I hope you see that I like this method in particular).
\begin{align*}
\E{R_n^2} = \E{(R_{n-1} + X_n + Y_n)^{2}},
\end{align*}
but $R_{n-1}$ and $X_n+Y_n$ are independent, and $\E{(X_n + Y_n)^2} = 1$. Using the recursion, $\E{R^2_n} = n$.
\end{solution}
\end{exercise}


\setcounter{theorem}{57}
\begin{exercise}
BH.7.58.
This is a totally great exercise. First solve it yourself. In the solution, I'll explain why, in particular how to relate the concept of covariance to the determinant of a matrix.

\begin{hint}
a. Expand the brackets in the expression for the sample variance $r$ to see that
\begin{align*}
r = 1/n \sum_i x_i y_i - \bar x \bar y.
\end{align*}
Next, we choose with probability $1/n$ one the points $(x_i, y_{i})$.  Under this probability, $\E{X Y} = 1/n \sum_i x_i y_i$, $\E X = \bar x, \E Y = \bar y$. So, how do $\cov{X,Y}$ and $r$ relate?


b. Expand the brackets and use iid and linearity properties to show that the expected area spanned by two random points $(X,Y)$ and $(\tilde X, \tilde Y)$ satisfies
\begin{align*}
\E{(X-\tilde X)(Y-\tilde Y)} = 2\cov{X,Y}.
\end{align*}

\end{hint}
\begin{solution}

b. Use the hint. Then, if we choose two points at random from the sample, then $(x_i-x_j)(y_i-y_j)$ is the area spanned by these  two points.
More generally, I have $n$ choices for my first point, and also $n$ choices for the second point (if both points are the same, the area of the rectangle is 0, so we don't have to exclude such choices).
Hence, the expected area of the rectangle spanned by the two random points $(X,Y)$ and $(\tilde X, \tilde Y)$ is
\begin{align*}
\frac 1 {n^2} \sum_{i,j} (x_i-x_j)(y_i-y_j).
\end{align*}
Simplify this to show that
\begin{align*}
2 \frac 1 n \sum_i x_i y_i - 2 \bar x \bar y = 2 r
\end{align*}
Hence, by part a., the expected area is twice the covariance.

Why is $\cov{X,a}= 0$ for $a$ a constant? Because the `area' of rectangles, all with the same \(y\)-coordinate, is zero, i.e., they lie on a line.

c.  This is the part of the exercise that explains what the above is all about.
Since there is a direct relation between covariance and area, we can use geometric arguments to derive (and memorize!) all properties of covariance! Write property i. of covariance  as $\cov{X,Y} = \cov{Y,X}$. Suppose I flip the \(x\) and \(y\)-axis, does the area of a rectangle change?  For property ii., what happens to the area of rectangle if you stretch the sides? For property iii., realize that this is just a shift of a rectangle that leaves its area invariant. For property iv., what happens to the area if you put an extra rectangle on top or to the right?

BTW, property iii. follows directly from property iv. In iv., take $W_3$ equal to a constant $a_2$, in other words $\P{W_3=a_2}=1$. We know that $\cov{X, a} = 0$ for a constant $a$.

Here are  some final remarks.

Let's put all the above in a very general frame.  The covariance has a number of interesting properties:
\begin{enumerate}
\item  It is bilinear, that is, the covariance is linear in both arguments. The linearity in the first argument means that $\cov{X+Y, Z}=\cov{X,Z}+\cov{Y,Z}$ and $\cov{a X, Z}=a\cov{X,Z}$ for $a\in \R$. The linearity in the second argument means that $\cov{X, Y+Z}=\cov{X,Y}+\cov{X,Z}$ and $\cov{X, a Z}=a\cov{X,Z}$ for $a\in \R$.
\item It is symmetric: $\cov{X, Y}=\cov{Y,X}$, from which we define $\V X = \cov{X,X}$.
\item  $\cov{X,a} = 0$ for all $a\in R$.
\end{enumerate}
If you memorize the first two properties of covariance, all the rest follows.

Now we do some geometry. Take three vectors $x,y, z\in \R^2$ (it's easy to generalize to $\R^n)$. Then we know that the area $D(x,y)$ of the parallelogram spanned by vectors $x$ and $y$  satisfies the following properties.
\begin{enumerate}
\item  Area is bilinear. The linearity in the first argument means that $D(x+y, z) = D(x, z) + D(y, z)$ and $D(ax, z)=a D(x, z)$ for $a\in \R$. (Just make a drawing to convince you about this.) The linearity in the second argument means that  $D(x, y+z) = D(x, y) + D(x, z)$  and $D(x, a z)=a D(x, z)$ for $a\in \R$.
\item  $D(x,x)$ = 0; there is no area between $x$ and $x$.
\item $D( (1,0), (0,1)) = 1$; the area of the square with side 1 is 1.
\end{enumerate}
In fact, thex first property means that stretching vectors and stacking parallelograms result in stretching and adding areas.
The second says that the area of a parallelogram spanned by two parallel vectors is zero. The third specifies that the area of the unit square is 1.

Now it can be proven that there exists just one function $D$ that satisfies these properties. In fact, this is the determinant of the matrix with as columns the vectors that span the parallellogram. Moreover, it can be shown that the second property can be replaced by the skew-symmetric property: $D(x,y) = - D(x,y)$.
(Note that $D(x,x) = -D(x,x) \implies 2 D(x,x) = 0 \implies D(x,x) = 0$.)

Let us use the properties to compute the area of a parallelogram spanned by the vectors $x = (a,b)$ and $y =(c,d)$ in 2D. Then
\begin{align*}
D(x,y) &= D((a,b), (c,d)) =  D(a (1,0)) + b(0,1), c(1,0) + d(0,1)) \\
&= ad D((1,0), (0,1)) + b c D((0,1), (1,0)) = ad - b c,
\end{align*}
where we use bilinearity in the first step, and skew-symmetry in the second and third. And this is indeed the determinant of the matrix with $x$ and $y$ as columns.

So, all in all, this is what I remembered throughout the years: the covariance and the determinant are bi-linear forms, the first is symmetric, the second skew- (or anti-)symmetric.

Finally, I don't see why the areas of the rectangles have to have a sign in this problem. Interestingly, for the determinant, the areas of the parallelograms do have to have a sign to make the concept useful for physics.
\end{solution}
\end{exercise}

\setcounter{theorem}{58}
\begin{exercise}
BH.7.59.
Read this exercise, then read (and do) BH.5.53 for some further background.
You'll encounter these topics countless times in other courses!
The final answer is really nice and intuitive.

\begin{hint}
a. Use that expectation is linear.

b. Read the entire exercise in its entirety before trying to solve it. In this case trying to solve c.\/ seems simpler because of the extra iid assumption. You  might want to use this to formulate some simple guesses.

Thus first part c. It is given that the $X_i$ and $Y_j$ are iid. Then, if I could improve the estimator $\hat \theta$ by splitting the measurements into two sets $X_i$ and $Y_j$, then I would certainly do that.
And not only I would do that; anybody in his right mind would do that.
But, I never heard of this idea, and I am sure you have neither, so this must be impossible (because if it would, people would have been using this trick for ages.)
Hence, we can place this in the context of the maxim: `we cannot obtain information for free'.
For this case, this must imply that splitting iid measurements into smaller sets cannot help with improving the estimator. What does this idea imply for the weights?


Part b, continued. I always try to solve the problem myself without a hint. This lead to the following considerations, which gave me quite a bit of extra understanding beyond the problem itself.  As a next piece of advice, before doing hard work, I prefer to look at some corner cases to acquire some intuitive understanding. I also use the rvs of Part c.

Suppose  that $v_2:=\V{Y_j} = 0$, but $v_1 := \V{X_i} > 0$. (For instance, $Y_j$ is the $j$th measurement of a perfect machine and $X_j$ of an imperfect machine.)
Then we know that the set $\{Y_j\}$ forms a set of perfect measurements.
But then I am not interested in the $\{X_i\}$ measurements anymore; why should I as I have the perfect measurements $\{Y_j\}$ at my disposal.
So, then I put $w_1=0$, because I don't want the $\{X_i\}$ measurements to pollute my estimator.
In other words, the final result should be such that $v_2=0 \implies w_{1}=0$, and vice versa.


More generally, I learned from this  corner case that I want this for the final result:  when $v_2<v_{1} \implies w_1 < w_{2}$, and vice versa.

How would you choose the weights such that this requirement is satisfied, but also the condition imposed by Part c.?
\end{hint}


\begin{solution}
a. Follows directly from the hint.

Check the hint!

c.  If $X_i$ and $Y_j$ are iid, it must be that $w_{1} = n/(n+m)$.

b. Can we make some further progress, just by keeping a clear mind?
Well, in fact we can by using our insights of part c.
If we have $n+m$ iid measurements of which we call $n$ measurements of type $X_i$, and $m$ of type $Y_{j}$, then
\begin{align*}
\V{\hat \theta_{1}}  =  \E{\left(\frac{1}{n}\sum_{i}X_{i} - \theta\right)^2} = n^{-2}\E{\left(\sum_i (X_{i}-\theta)\right)^{2}} = n^{-2}\V{\sum_i X_i} = \V{X_1}/n = \sigma^{2}/n.
\end{align*}
So, $n=\sigma^{2}/\V{\hat \theta_1}$, and likewise $m=\sigma^{2}/\V{\hat \theta_2}$. Finally, plug this into our earlier expression for $w_1$ to  get
\begin{align*}
w_1 = \frac n {n+m} = \frac{\sigma^2/\V{\hat \theta_1}}{\sigma^2/\V{\hat \theta_1} + \sigma^2/\V{\theta_2}} = \frac{\V{\hat \theta_2}}{\V{\hat \theta_1} + \V{\theta_2}}.
\end{align*}
If we check our earlier insight, then we see that if $\V{Y_j}=0$, then $\V{\theta_2}=0$, hence $w_1=0$ in that case. This is precisely what we wanted.

Let us finally use the  hint of BH to check that the above expression for $w_1$ is correct.
\begin{align*}
\E{(\hat \theta -\theta)^{2}} =
\E{(w_1(\hat \theta_{1} - \theta) + w_2(\hat \theta_{2}-\theta))^{2}} =
\V{w_1 \hat \theta_{1}} + \V{w_2 \hat \theta_{2}},
\end{align*}
by independence. Take the $w$'s out of the variances, then write $w_2=1-w_1$, take $\partial_{w_1}$ of the expression,  set the result to 0, and solve for $w_1$. You'll get the above expression.
\end{solution}
\end{exercise}

\setcounter{theorem}{70}
\begin{exercise}
BH.7.71.

\begin{hint}

b. The people in the sample of size $n$ with an $A$ is $X_1+X_2$. But this is the same as $n-X_3$. Hence, what is $\P{X_3=n-i}$?


c. I found this a hard problem.
Here is my hint based on recursion.
Let $S_n$ be the number of $A$s in $n$ individuals.
We want to know $f_n(i) = \P{S_n=i}$.
A simple recursive idea, i.e., one-step analysis by conditioning on the phenotype of the $n$th person, gives that
\begin{align*}
f_n(i)=f_{n-1}(i-2) p^2 + f_{n-1}(i-1) 2p q + f_{n-1}(i)q^2,
\end{align*}
with $q=1-p$ as always. Now I was a bit stuck, but just to try to see whether I could see some structure, I tried a simpler case, namely, a recursion for the binomial distribution. Derive this, and then use this to solve the problem.


d. It is easiest to work with $f(p) = \log \P{X_1=k, X_2=l, X_3 = m}$, where $\P{X_1=k, X_2=l, X_3 = m}$ follows from a., and then differentiate with respect to $p$.

e. Follow the same scheme as for d.
\end{hint}

\begin{solution}
a. Multinomial.

b. With the hint we end up at $X_1+X_2\sim \Bin{n, p^2+2p(1-p)}$.

c. Here is a short intermezzo on finding a recursion for the sum of a number of Bernouilli rvs.  Let $S_n$ be the number of successes in the binomial, and write $g_n(i) = \P{S_n=i}$ for this case.
Then,
\begin{align*}
g_n(i)&= g_{n-1}(i-1)p +  g_{n-1}(i)q \\
&= (g_{n-2}(i-2)p + g_{n-2}(i-1)q)p + (g_{n-2}(i-1)p+g_{n-2}(i)q)q \\
&= g_{n-2}(i-2)p^2 + g_{n-2}(i-1)2p q + g_{n-2}(i)q^{2}.
\end{align*}
I also know that $g_n(i) = {n \choose i} p^iq^{n-i}$.
End of intermezzo.

Now compare the recursion with $f_n(i)$ for the genes tox the expression for the binomial.
They are nearly the same, except that in the genes case, the `n' seems to run twice as fast.
I then tried the guess $f_n(i) = {2n \choose i} p^i q^{2n-i}$.
For you, plug it in, and show that it works.

So, what was my overall approach?
I used recursion, but got stuck.
Then I used recursion for a simpler case whose solution I know by heart.
I compared the recursions for both cases to see whether I could recognize a pattern.
This lead me to a guess, which I verified by plugging it in.
Using recursion is not guaranteed to work, of course, but often it's worth a try.

Now, looking back, I realize that it is as if individual $n$ adds the outcome of two coin flips (with values in $AA$, $Aa$ or $a a$) to the sum $S_{n}$ of $A'$s. For you to solve: what is the distribution of two coin flips? Next, $S_n$ is just the sum of $n$ individual `double coin flips'. Hence, what must the distribution of $S_n$ be?

d. It is easiest to work with $f(p) = \log \P{X_1=k, X_2=l, X_3 = m}$. With part a. this can be written as
\begin{align*}
f(p) = C + (2k+l)\log p + (l+2m)\log(1-p),
\end{align*}
where $C$ is a constant (the log of the normalization constant). (BTW, with this you can check your answer for part a.)
Compute $\d f(p)/\d p = 0$, because at this $p$, $\log f$, hence $f$ itself, is maximal.  Observe that $C$ drops out of the computation, because when differentiating, it disappears.


e. Now we like to know what $p$  maximizes $\P{X_3=n-i}$. Take $g(q) = \log \P{X_3 = n-i}$, then
\begin{align*}
g(q) = C + i \log (1-q^{2}) + 2 (n-i)\log q.
\end{align*}
(With this, check your answer of part b.) Again, take the derivative (with respect to $q$), and solve for $q$.
\end{solution}
\end{exercise}

\setcounter{theorem}{85}
\begin{exercise}
BH.7.86. The concepts discussed here are a standard part of the education of GPs (i.e., medical doctors), and in data science in general.
\begin{hint}
The challenge for you is to try to understand the mathematics behind these concepts.
Read the exercise a number of times. I found it quite difficult to capture the concepts in formulas. (I solved it once. After two weeks,  I tried to solve it again, and found it just as hard as the first time.) Once you have the model, the technical part itself is simple.
\end{hint}
\begin{solution}
a. It is given that $\P{T\leq t\given D=1} = G(t)$ and $\P{T\leq t\given D=0} = H(t)$. From Theorem 5.3.1.i,  we have that we can associate a rv. to a CDF F. Sometimes we say that the CDF $F$ /induces/ a rv. $X$.  So let us use this here to say that $G$ induces the rv. $T_1$ and $H$ induces $T_0$.
So the /sensitivity/ is $\P{T_1>t_0} = 1-G(t_0)$ and the /specificity/ is $\P{T_1<t_0} = H(t_0)$.

To make the ROC plot, I first made two plots, one of the sensitivity and the other for 1 minus the specificity, i.e., $1-H(t_0)$.
Then, in the ROC plot, we put a specificity of $s$ on the \(x\)-axis, then we search for a $t$ such that $1-H(t) = s$, and then we plug this $t$ into $1-G(t)$ to get the sensitivity.
To help you understand this better, check that $s=0 \implies t = b \implies 1-G(t) = 0$.
Moreover, check that $s=1\implies t=a \implies 1- G(t) = 1$.
Hence, the ROC curve starts in the origin and stops at the point $(1,1)$.

With this insight, the area under the ROC curve can be written as
\begin{align*}
\int_0^1 (1-G(H^{-1}(1-s))) \d s  =
1 - \int_0^1 G(H^{-1}(1-s)) \d s  =
1 - \int_a^b G(t) h(t) \d t,
\end{align*}
where, in the last step, we use the 1D change of variable $H(t)=1-s \implies h(t) \d t = -\d s$. It remains to  interpret the integral, so let's plug in the definitions:
\begin{align*}
\int_a^b G(t) h(t) \d t =
\int_a^b \P{T_1\leq t} f_{T_0}(t) \d t =
\int_a^b \P{T_1\leq T_0\given T_0 = t} f_{T_0}(t) \d t =  \P{T_{1\leq T_0}}.
\end{align*}

\end{solution}
\end{exercise}



\input{trailer}
